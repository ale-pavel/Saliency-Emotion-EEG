{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6430c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a97d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d90fb91",
   "metadata": {},
   "source": [
    "## DE Features (one subject)\n",
    "\n",
    "https://github.com/ynulonger/DE_CNN\n",
    "\n",
    "https://www.researchgate.net/publication/328504085_Continuous_Convolutional_Neural_Network_with_3D_Input_for_EEG-Based_Emotion_Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1894301",
   "metadata": {},
   "outputs": [],
   "source": [
    "deap_de_path = '../../methods/DE_CNN/1D_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a7a50f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__header__\n",
      "__version__\n",
      "__globals__\n",
      "base_data\n",
      "data\n",
      "valence_labels\n",
      "arousal_labels\n"
     ]
    }
   ],
   "source": [
    "s0 = scipy.io.loadmat(deap_de_path + 'DE_s01.mat')\n",
    "for i, key in enumerate(s0):\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ddea8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = s0['data']\n",
    "y_0_valence = s0['valence_labels']\n",
    "y_0_arousal = s0['arousal_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6d0a295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 4, 32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dfb76c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_0_valence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7756617a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(y_0_valence).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c6de5",
   "metadata": {},
   "source": [
    "### Merge all subjects' features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b64abb",
   "metadata": {},
   "source": [
    "Subjects number and indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38f48ed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 2399\n",
      "2 2400 4799\n",
      "3 4800 7199\n",
      "4 7200 9599\n",
      "5 9600 11999\n",
      "6 12000 14399\n",
      "7 14400 16799\n",
      "8 16800 19199\n",
      "9 19200 21599\n",
      "10 21600 23999\n",
      "11 24000 26399\n",
      "12 26400 28799\n",
      "13 28800 31199\n",
      "14 31200 33599\n",
      "15 33600 35999\n",
      "16 36000 38399\n",
      "17 38400 40799\n",
      "18 40800 43199\n",
      "19 43200 45599\n",
      "20 45600 47999\n",
      "21 48000 50399\n",
      "22 50400 52799\n",
      "23 52800 55199\n",
      "24 55200 57599\n",
      "25 57600 59999\n",
      "26 60000 62399\n",
      "27 62400 64799\n",
      "28 64800 67199\n",
      "29 67200 69599\n",
      "30 69600 71999\n",
      "31 72000 74399\n",
      "32 74400 76799\n"
     ]
    }
   ],
   "source": [
    "c = 2400\n",
    "\n",
    "for idx in range(32):\n",
    "    print(idx+1, idx*c, (idx+1)*c-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b96ea15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../methods/DE_CNN/1D_dataset/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deap_de_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5a5f1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_de_cnn_features = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd55a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_de_cnn_features:\n",
    "    de_cnn_features = np.empty((2400 * 32, 4, 32))\n",
    "    de_cnn_y_valence = np.empty((2400 * 32, 1))\n",
    "    de_cnn_y_arousal = np.empty((2400 * 32, 1))\n",
    "    \n",
    "    for i in range(1, 33):  # Subjects 1-32 in DEAP\n",
    "        subj_data = scipy.io.loadmat(deap_de_path + f'DE_s{i:02}.mat')\n",
    "\n",
    "        Xi_de = subj_data['data']\n",
    "        yi_valence = np.transpose(subj_data['valence_labels'])\n",
    "        yi_arousal = np.transpose(subj_data['arousal_labels'])\n",
    "        \n",
    "        idx = i-1  # indexing 0-31 for arrays\n",
    "        c = 2400  # size of each subject's trials*1s_windows\n",
    "\n",
    "        # efficient assigning, not really needed, could use np.append\n",
    "        de_cnn_features[idx*c:(idx+1)*c] = Xi_de\n",
    "        de_cnn_y_valence[idx*c:(idx+1)*c] = yi_valence\n",
    "        de_cnn_y_arousal[idx*c:(idx+1)*c] = yi_arousal\n",
    "        \n",
    "        save_dict = {'data': de_cnn_features, \n",
    "                     'valence_labels': de_cnn_y_valence, \n",
    "                     'arousal_labels': de_cnn_y_arousal}\n",
    "         \n",
    "    np.save(deap_de_path + 'DE_merged.npy', save_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcfb2001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from file.\n",
      "(76800, 4, 32)\n",
      "(76800, 1)\n",
      "(76800, 1)\n"
     ]
    }
   ],
   "source": [
    "if not merge_de_cnn_features:\n",
    "    de_cnn_merged = np.load(deap_de_path + 'DE_merged.npy', allow_pickle=True).item()\n",
    "    de_cnn_features = de_cnn_merged['data']\n",
    "    de_cnn_y_valence = de_cnn_merged['valence_labels']\n",
    "    de_cnn_y_arousal = de_cnn_merged['arousal_labels']\n",
    "    \n",
    "    print('Loaded from file.')\n",
    "    print(de_cnn_features.shape)\n",
    "    print(de_cnn_y_valence.shape)\n",
    "    print(de_cnn_y_arousal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0af6c0",
   "metadata": {},
   "source": [
    "## Load DE Features (all subjects)\n",
    "\n",
    "https://github.com/gzoumpourlis/DEAP_MNE_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "309919d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_features_path = '../../preprocessing/DEAP_MNE_preprocessing/features_new/de_feats_merged.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "529ded37",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_features = np.load(de_features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de730857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 32, 5, 232)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2dbdd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "deap_path = '../../datasets/DEAP/merged/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "798fae6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.load(deap_path + 'deap_full_labels.npy')\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391535f0",
   "metadata": {},
   "source": [
    "Column 0 is Valence, 1 is Arousal, 2 is quadrants notation (HAHV, HALV, LAHV, LALV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1973329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valence = 0\n",
    "arousal = 1\n",
    "quadrants = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f62053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y[:, valence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09519b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f63235",
   "metadata": {},
   "source": [
    "## Define DEAP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d279b18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T23:18:14.892936Z",
     "start_time": "2022-11-16T23:18:14.886230Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44850796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T23:18:14.903641Z",
     "start_time": "2022-11-16T23:18:14.895007Z"
    }
   },
   "outputs": [],
   "source": [
    "class DEAPDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.X = data\n",
    "        self.y = labels\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed4d756",
   "metadata": {},
   "source": [
    "Either use 'de_features' (DEAP_MNE_preprocessing) or 'de_cnn_features' (DE_CNN)\n",
    "\n",
    "Labels are 'y' or 'de_cnn_y_valence/de_cnn_y_arousal' respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13655227",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_de_cnn = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1977f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_de_cnn:\n",
    "    deap_dataset = DEAPDataset(de_cnn_features, de_cnn_y_valence)\n",
    "else:\n",
    "    deap_dataset = DEAPDataset(de_features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3076444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deap_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81909eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 32, 5, 232)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = de_features[:64] # small batch deap_mne_preprocessing\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fafc0d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 4, 32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = de_cnn_features[:64] # small batch de_cnn\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26d9426",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "caf16a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99e08902",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f668ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75 0.25\n",
      "57600 19200\n"
     ]
    }
   ],
   "source": [
    "train_split = 0.75\n",
    "test_split = 0.25\n",
    "\n",
    "train_n_elems = int(train_split * len(deap_dataset))\n",
    "test_n_elems = int(test_split * len(deap_dataset))\n",
    "\n",
    "print(train_split, test_split)\n",
    "print(train_n_elems, test_n_elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5c4c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d05de09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deap_train, deap_test = random_split(deap_dataset, [train_n_elems, test_n_elems])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bebe5f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deap_train = deap_dataset[:train_n_elems]\n",
    "# deap_test = deap_dataset[train_n_elems:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "940783ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57600"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deap_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b00e2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(deap_train, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_dataloader = DataLoader(deap_test, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e1a57c",
   "metadata": {},
   "source": [
    "## Define BiHDM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3c4014e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cb6597ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models_DEAP import BiHDM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1084561d",
   "metadata": {},
   "source": [
    "### BiHDM Initialization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b784f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "input_size = 4\n",
    "n_classes = 1\n",
    "\n",
    "# batch_first=False\n",
    "# bidirectional=False\n",
    "\n",
    "fc_input=448\n",
    "fc_hidden=96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe6a55ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiHDM(hidden_size=hidden_size, num_layers=num_layers, input_size=input_size, \n",
    "              fc_input=fc_input, fc_hidden=fc_hidden, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0a128b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiHDM(\n",
       "  (RNN_VL): RNN(4, 32, num_layers=2)\n",
       "  (RNN_VR): RNN(4, 32, num_layers=2)\n",
       "  (RNN_V): RNN(32, 32, num_layers=2)\n",
       "  (RNN_HL): RNN(4, 32, num_layers=2)\n",
       "  (RNN_HR): RNN(4, 32, num_layers=2)\n",
       "  (RNN_H): RNN(32, 32, num_layers=2)\n",
       "  (fc_v): Sequential(\n",
       "    (0): Linear(in_features=448, out_features=96, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc_h): Sequential(\n",
       "    (0): Linear(in_features=448, out_features=96, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc_c): Sequential(\n",
       "    (0): Linear(in_features=96, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c22c788",
   "metadata": {},
   "source": [
    "## Training BiHDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "55cf0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "282fd287",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "betas=(0.9, 0.999)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "77240e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f09f20b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_steps = len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5a89b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../BiHDM-Model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0dc7c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = False\n",
    "resume_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e1635a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10696408897638321\n"
     ]
    }
   ],
   "source": [
    "if not skip_training:\n",
    "    if resume_training:\n",
    "        #checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict']) \n",
    "        min_loss = checkpoint['min_loss']\n",
    "    else:\n",
    "        min_loss = 1000\n",
    "        \n",
    "print(min_loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7dbbcebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Step [600/1800], Loss: 0.516962\n",
      "Epoch [1/200], Step [1200/1800], Loss: 0.573717\n",
      "Epoch [1/200], Step [1800/1800], Loss: 0.341625\n",
      "Epoch [2/200], Step [600/1800], Loss: 0.608397\n",
      "Epoch [2/200], Step [1200/1800], Loss: 0.646369\n",
      "Epoch [2/200], Step [1800/1800], Loss: 0.301434\n",
      "Epoch [3/200], Step [600/1800], Loss: 0.544228\n",
      "Epoch [3/200], Step [1200/1800], Loss: 0.680549\n",
      "Epoch [3/200], Step [1800/1800], Loss: 0.309841\n",
      "Epoch [4/200], Step [600/1800], Loss: 0.517641\n",
      "Epoch [4/200], Step [1200/1800], Loss: 0.733751\n",
      "Epoch [4/200], Step [1800/1800], Loss: 0.272661\n",
      "Epoch [5/200], Step [600/1800], Loss: 0.507598\n",
      "Epoch [5/200], Step [1200/1800], Loss: 0.658273\n",
      "Epoch [5/200], Step [1800/1800], Loss: 0.341298\n",
      "Epoch [6/200], Step [600/1800], Loss: 0.476387\n",
      "Epoch [6/200], Step [1200/1800], Loss: 0.651253\n",
      "Epoch [6/200], Step [1800/1800], Loss: 0.279729\n",
      "Epoch [7/200], Step [600/1800], Loss: 0.667901\n",
      "Epoch [7/200], Step [1200/1800], Loss: 0.596846\n",
      "Epoch [7/200], Step [1800/1800], Loss: 0.294546\n",
      "Epoch [8/200], Step [600/1800], Loss: 0.649432\n",
      "Epoch [8/200], Step [1200/1800], Loss: 0.541887\n",
      "Epoch [8/200], Step [1800/1800], Loss: 0.242308\n",
      "Epoch [9/200], Step [600/1800], Loss: 0.490882\n",
      "Epoch [9/200], Step [1200/1800], Loss: 0.462900\n",
      "Epoch [9/200], Step [1800/1800], Loss: 0.319177\n",
      "Epoch [10/200], Step [600/1800], Loss: 0.512029\n",
      "Epoch [10/200], Step [1200/1800], Loss: 0.542094\n",
      "Epoch [10/200], Step [1800/1800], Loss: 0.239162\n",
      "Epoch [11/200], Step [600/1800], Loss: 0.475623\n",
      "Saved checkpoint - loss: 0.106028\n",
      "Epoch [11/200], Step [1200/1800], Loss: 0.605381\n",
      "Epoch [11/200], Step [1800/1800], Loss: 0.346756\n",
      "Epoch [12/200], Step [600/1800], Loss: 0.556689\n",
      "Epoch [12/200], Step [1200/1800], Loss: 0.415374\n",
      "Epoch [12/200], Step [1800/1800], Loss: 0.259003\n",
      "Epoch [13/200], Step [600/1800], Loss: 0.544544\n",
      "Epoch [13/200], Step [1200/1800], Loss: 0.406639\n",
      "Epoch [13/200], Step [1800/1800], Loss: 0.265423\n",
      "Saved checkpoint - loss: 0.090768\n",
      "Epoch [14/200], Step [600/1800], Loss: 0.565068\n",
      "Epoch [14/200], Step [1200/1800], Loss: 0.326909\n",
      "Epoch [14/200], Step [1800/1800], Loss: 0.306584\n",
      "Epoch [15/200], Step [600/1800], Loss: 0.431651\n",
      "Epoch [15/200], Step [1200/1800], Loss: 0.366580\n",
      "Epoch [15/200], Step [1800/1800], Loss: 0.313541\n",
      "Epoch [16/200], Step [600/1800], Loss: 0.453891\n",
      "Epoch [16/200], Step [1200/1800], Loss: 0.435034\n",
      "Epoch [16/200], Step [1800/1800], Loss: 0.330045\n",
      "Saved checkpoint - loss: 0.085622\n",
      "Saved checkpoint - loss: 0.081151\n",
      "Epoch [17/200], Step [600/1800], Loss: 0.407278\n",
      "Epoch [17/200], Step [1200/1800], Loss: 0.534582\n",
      "Epoch [17/200], Step [1800/1800], Loss: 0.230404\n",
      "Epoch [18/200], Step [600/1800], Loss: 0.435403\n",
      "Epoch [18/200], Step [1200/1800], Loss: 0.417635\n",
      "Epoch [18/200], Step [1800/1800], Loss: 0.260226\n",
      "Epoch [19/200], Step [600/1800], Loss: 0.456362\n",
      "Epoch [19/200], Step [1200/1800], Loss: 0.476247\n",
      "Epoch [19/200], Step [1800/1800], Loss: 0.319808\n",
      "Epoch [20/200], Step [600/1800], Loss: 0.516538\n",
      "Epoch [20/200], Step [1200/1800], Loss: 0.564208\n",
      "Epoch [20/200], Step [1800/1800], Loss: 0.300460\n",
      "Epoch [21/200], Step [600/1800], Loss: 0.410837\n",
      "Epoch [21/200], Step [1200/1800], Loss: 0.401814\n",
      "Epoch [21/200], Step [1800/1800], Loss: 0.418460\n",
      "Epoch [22/200], Step [600/1800], Loss: 0.525771\n",
      "Epoch [22/200], Step [1200/1800], Loss: 0.426521\n",
      "Epoch [22/200], Step [1800/1800], Loss: 0.306113\n",
      "Epoch [23/200], Step [600/1800], Loss: 0.279429\n",
      "Epoch [23/200], Step [1200/1800], Loss: 0.418520\n",
      "Epoch [23/200], Step [1800/1800], Loss: 0.355182\n",
      "Epoch [24/200], Step [600/1800], Loss: 0.352788\n",
      "Epoch [24/200], Step [1200/1800], Loss: 0.458673\n",
      "Epoch [24/200], Step [1800/1800], Loss: 0.305359\n",
      "Epoch [25/200], Step [600/1800], Loss: 0.492012\n",
      "Epoch [25/200], Step [1200/1800], Loss: 0.565369\n",
      "Saved checkpoint - loss: 0.078156\n",
      "Epoch [25/200], Step [1800/1800], Loss: 0.254900\n",
      "Epoch [26/200], Step [600/1800], Loss: 0.393630\n",
      "Epoch [26/200], Step [1200/1800], Loss: 0.503191\n",
      "Epoch [26/200], Step [1800/1800], Loss: 0.338978\n",
      "Epoch [27/200], Step [600/1800], Loss: 0.450215\n",
      "Epoch [27/200], Step [1200/1800], Loss: 0.317197\n",
      "Epoch [27/200], Step [1800/1800], Loss: 0.251974\n",
      "Epoch [28/200], Step [600/1800], Loss: 0.488181\n",
      "Epoch [28/200], Step [1200/1800], Loss: 0.335608\n",
      "Epoch [28/200], Step [1800/1800], Loss: 0.297956\n",
      "Saved checkpoint - loss: 0.067723\n",
      "Epoch [29/200], Step [600/1800], Loss: 0.487532\n",
      "Epoch [29/200], Step [1200/1800], Loss: 0.447348\n",
      "Saved checkpoint - loss: 0.063988\n",
      "Epoch [29/200], Step [1800/1800], Loss: 0.222997\n",
      "Epoch [30/200], Step [600/1800], Loss: 0.483561\n",
      "Epoch [30/200], Step [1200/1800], Loss: 0.379729\n",
      "Epoch [30/200], Step [1800/1800], Loss: 0.240448\n",
      "Epoch [31/200], Step [600/1800], Loss: 0.389168\n",
      "Epoch [31/200], Step [1200/1800], Loss: 0.429060\n",
      "Epoch [31/200], Step [1800/1800], Loss: 0.242849\n",
      "Epoch [32/200], Step [600/1800], Loss: 0.545728\n",
      "Epoch [32/200], Step [1200/1800], Loss: 0.422138\n",
      "Epoch [32/200], Step [1800/1800], Loss: 0.270193\n",
      "Epoch [33/200], Step [600/1800], Loss: 0.425775\n",
      "Epoch [33/200], Step [1200/1800], Loss: 0.549913\n",
      "Epoch [33/200], Step [1800/1800], Loss: 0.278179\n",
      "Epoch [34/200], Step [600/1800], Loss: 0.431188\n",
      "Epoch [34/200], Step [1200/1800], Loss: 0.676909\n",
      "Saved checkpoint - loss: 0.052206\n",
      "Epoch [34/200], Step [1800/1800], Loss: 0.217474\n",
      "Epoch [35/200], Step [600/1800], Loss: 0.495656\n",
      "Epoch [35/200], Step [1200/1800], Loss: 0.602679\n",
      "Epoch [35/200], Step [1800/1800], Loss: 0.219917\n",
      "Epoch [36/200], Step [600/1800], Loss: 0.432381\n",
      "Epoch [36/200], Step [1200/1800], Loss: 0.522768\n",
      "Epoch [36/200], Step [1800/1800], Loss: 0.200130\n",
      "Epoch [37/200], Step [600/1800], Loss: 0.439792\n",
      "Epoch [37/200], Step [1200/1800], Loss: 0.361816\n",
      "Epoch [37/200], Step [1800/1800], Loss: 0.207989\n",
      "Epoch [38/200], Step [600/1800], Loss: 0.415775\n",
      "Epoch [38/200], Step [1200/1800], Loss: 0.539003\n",
      "Epoch [38/200], Step [1800/1800], Loss: 0.222538\n",
      "Epoch [39/200], Step [600/1800], Loss: 0.389145\n",
      "Epoch [39/200], Step [1200/1800], Loss: 0.365285\n",
      "Epoch [39/200], Step [1800/1800], Loss: 0.289949\n",
      "Epoch [40/200], Step [600/1800], Loss: 0.398350\n",
      "Epoch [40/200], Step [1200/1800], Loss: 0.465654\n",
      "Saved checkpoint - loss: 0.050042\n",
      "Epoch [40/200], Step [1800/1800], Loss: 0.236709\n",
      "Epoch [41/200], Step [600/1800], Loss: 0.305313\n",
      "Epoch [41/200], Step [1200/1800], Loss: 0.375517\n",
      "Epoch [41/200], Step [1800/1800], Loss: 0.241524\n",
      "Epoch [42/200], Step [600/1800], Loss: 0.319796\n",
      "Epoch [42/200], Step [1200/1800], Loss: 0.458860\n",
      "Epoch [42/200], Step [1800/1800], Loss: 0.208567\n",
      "Epoch [43/200], Step [600/1800], Loss: 0.483461\n",
      "Epoch [43/200], Step [1200/1800], Loss: 0.478510\n",
      "Epoch [43/200], Step [1800/1800], Loss: 0.193575\n",
      "Epoch [44/200], Step [600/1800], Loss: 0.340601\n",
      "Epoch [44/200], Step [1200/1800], Loss: 0.358207\n",
      "Epoch [44/200], Step [1800/1800], Loss: 0.494620\n",
      "Epoch [45/200], Step [600/1800], Loss: 0.452885\n",
      "Epoch [45/200], Step [1200/1800], Loss: 0.450512\n",
      "Epoch [45/200], Step [1800/1800], Loss: 0.167642\n",
      "Epoch [46/200], Step [600/1800], Loss: 0.427390\n",
      "Epoch [46/200], Step [1200/1800], Loss: 0.384358\n",
      "Epoch [46/200], Step [1800/1800], Loss: 0.179780\n",
      "Epoch [47/200], Step [600/1800], Loss: 0.487386\n",
      "Epoch [47/200], Step [1200/1800], Loss: 0.295348\n",
      "Epoch [47/200], Step [1800/1800], Loss: 0.221181\n",
      "Epoch [48/200], Step [600/1800], Loss: 0.395376\n",
      "Epoch [48/200], Step [1200/1800], Loss: 0.505077\n",
      "Epoch [48/200], Step [1800/1800], Loss: 0.171456\n",
      "Epoch [49/200], Step [600/1800], Loss: 0.344485\n",
      "Epoch [49/200], Step [1200/1800], Loss: 0.409455\n",
      "Epoch [49/200], Step [1800/1800], Loss: 0.149647\n",
      "Epoch [50/200], Step [600/1800], Loss: 0.381612\n",
      "Epoch [50/200], Step [1200/1800], Loss: 0.360716\n",
      "Epoch [50/200], Step [1800/1800], Loss: 0.211927\n",
      "Epoch [51/200], Step [600/1800], Loss: 0.481661\n",
      "Epoch [51/200], Step [1200/1800], Loss: 0.363727\n",
      "Epoch [51/200], Step [1800/1800], Loss: 0.153617\n",
      "Epoch [52/200], Step [600/1800], Loss: 0.419421\n",
      "Epoch [52/200], Step [1200/1800], Loss: 0.221916\n",
      "Epoch [52/200], Step [1800/1800], Loss: 0.227328\n",
      "Epoch [53/200], Step [600/1800], Loss: 0.495623\n",
      "Saved checkpoint - loss: 0.045755\n",
      "Epoch [53/200], Step [1200/1800], Loss: 0.374914\n",
      "Epoch [53/200], Step [1800/1800], Loss: 0.331690\n",
      "Epoch [54/200], Step [600/1800], Loss: 0.530032\n",
      "Epoch [54/200], Step [1200/1800], Loss: 0.466080\n",
      "Epoch [54/200], Step [1800/1800], Loss: 0.194549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/200], Step [600/1800], Loss: 0.405220\n",
      "Epoch [55/200], Step [1200/1800], Loss: 0.525293\n",
      "Epoch [55/200], Step [1800/1800], Loss: 0.170830\n",
      "Epoch [56/200], Step [600/1800], Loss: 0.564863\n",
      "Epoch [56/200], Step [1200/1800], Loss: 0.244268\n",
      "Epoch [56/200], Step [1800/1800], Loss: 0.262767\n",
      "Epoch [57/200], Step [600/1800], Loss: 0.338160\n",
      "Epoch [57/200], Step [1200/1800], Loss: 0.368122\n",
      "Epoch [57/200], Step [1800/1800], Loss: 0.201050\n",
      "Epoch [58/200], Step [600/1800], Loss: 0.571804\n",
      "Epoch [58/200], Step [1200/1800], Loss: 0.297761\n",
      "Epoch [58/200], Step [1800/1800], Loss: 0.081477\n",
      "Epoch [59/200], Step [600/1800], Loss: 0.414562\n",
      "Epoch [59/200], Step [1200/1800], Loss: 0.352077\n",
      "Epoch [59/200], Step [1800/1800], Loss: 0.140315\n",
      "Epoch [60/200], Step [600/1800], Loss: 0.389959\n",
      "Epoch [60/200], Step [1200/1800], Loss: 0.427009\n",
      "Epoch [60/200], Step [1800/1800], Loss: 0.174780\n",
      "Epoch [61/200], Step [600/1800], Loss: 0.431297\n",
      "Epoch [61/200], Step [1200/1800], Loss: 0.328937\n",
      "Epoch [61/200], Step [1800/1800], Loss: 0.126111\n",
      "Epoch [62/200], Step [600/1800], Loss: 0.372531\n",
      "Epoch [62/200], Step [1200/1800], Loss: 0.530137\n",
      "Epoch [62/200], Step [1800/1800], Loss: 0.210614\n",
      "Epoch [63/200], Step [600/1800], Loss: 0.739939\n",
      "Epoch [63/200], Step [1200/1800], Loss: 0.194201\n",
      "Saved checkpoint - loss: 0.029001\n",
      "Epoch [63/200], Step [1800/1800], Loss: 0.255730\n",
      "Epoch [64/200], Step [600/1800], Loss: 0.570364\n",
      "Epoch [64/200], Step [1200/1800], Loss: 0.404201\n",
      "Epoch [64/200], Step [1800/1800], Loss: 0.286982\n",
      "Epoch [65/200], Step [600/1800], Loss: 0.537874\n",
      "Epoch [65/200], Step [1200/1800], Loss: 0.456515\n",
      "Epoch [65/200], Step [1800/1800], Loss: 0.210421\n",
      "Epoch [66/200], Step [600/1800], Loss: 0.415438\n",
      "Epoch [66/200], Step [1200/1800], Loss: 0.270033\n",
      "Epoch [66/200], Step [1800/1800], Loss: 0.316652\n",
      "Epoch [67/200], Step [600/1800], Loss: 0.585758\n",
      "Epoch [67/200], Step [1200/1800], Loss: 0.346730\n",
      "Epoch [67/200], Step [1800/1800], Loss: 0.180090\n",
      "Epoch [68/200], Step [600/1800], Loss: 0.315111\n",
      "Epoch [68/200], Step [1200/1800], Loss: 0.481008\n",
      "Epoch [68/200], Step [1800/1800], Loss: 0.282689\n",
      "Epoch [69/200], Step [600/1800], Loss: 0.253691\n",
      "Epoch [69/200], Step [1200/1800], Loss: 0.398947\n",
      "Epoch [69/200], Step [1800/1800], Loss: 0.338547\n",
      "Epoch [70/200], Step [600/1800], Loss: 0.576001\n",
      "Epoch [70/200], Step [1200/1800], Loss: 0.330782\n",
      "Epoch [70/200], Step [1800/1800], Loss: 0.426014\n",
      "Epoch [71/200], Step [600/1800], Loss: 0.322199\n",
      "Epoch [71/200], Step [1200/1800], Loss: 0.324347\n",
      "Epoch [71/200], Step [1800/1800], Loss: 0.321310\n",
      "Epoch [72/200], Step [600/1800], Loss: 0.361126\n",
      "Epoch [72/200], Step [1200/1800], Loss: 0.335656\n",
      "Epoch [72/200], Step [1800/1800], Loss: 0.278947\n",
      "Epoch [73/200], Step [600/1800], Loss: 0.297080\n",
      "Epoch [73/200], Step [1200/1800], Loss: 0.293995\n",
      "Epoch [73/200], Step [1800/1800], Loss: 0.227068\n",
      "Epoch [74/200], Step [600/1800], Loss: 0.230420\n",
      "Epoch [74/200], Step [1200/1800], Loss: 0.234439\n",
      "Epoch [74/200], Step [1800/1800], Loss: 0.148917\n",
      "Epoch [75/200], Step [600/1800], Loss: 0.400362\n",
      "Epoch [75/200], Step [1200/1800], Loss: 0.413377\n",
      "Epoch [75/200], Step [1800/1800], Loss: 0.256048\n",
      "Epoch [76/200], Step [600/1800], Loss: 0.401548\n",
      "Epoch [76/200], Step [1200/1800], Loss: 0.383127\n",
      "Epoch [76/200], Step [1800/1800], Loss: 0.259792\n",
      "Epoch [77/200], Step [600/1800], Loss: 0.279566\n",
      "Epoch [77/200], Step [1200/1800], Loss: 0.236915\n",
      "Epoch [77/200], Step [1800/1800], Loss: 0.281310\n",
      "Epoch [78/200], Step [600/1800], Loss: 0.302960\n",
      "Epoch [78/200], Step [1200/1800], Loss: 0.257669\n",
      "Epoch [78/200], Step [1800/1800], Loss: 0.233576\n",
      "Epoch [79/200], Step [600/1800], Loss: 0.270549\n",
      "Epoch [79/200], Step [1200/1800], Loss: 0.334837\n",
      "Epoch [79/200], Step [1800/1800], Loss: 0.400680\n",
      "Epoch [80/200], Step [600/1800], Loss: 0.510236\n",
      "Epoch [80/200], Step [1200/1800], Loss: 0.294415\n",
      "Epoch [80/200], Step [1800/1800], Loss: 0.388586\n",
      "Epoch [81/200], Step [600/1800], Loss: 0.306847\n",
      "Epoch [81/200], Step [1200/1800], Loss: 0.321933\n",
      "Epoch [81/200], Step [1800/1800], Loss: 0.305323\n",
      "Saved checkpoint - loss: 0.026765\n",
      "Epoch [82/200], Step [600/1800], Loss: 0.346103\n",
      "Epoch [82/200], Step [1200/1800], Loss: 0.243063\n",
      "Epoch [82/200], Step [1800/1800], Loss: 0.336912\n",
      "Epoch [83/200], Step [600/1800], Loss: 0.396403\n",
      "Epoch [83/200], Step [1200/1800], Loss: 0.466127\n",
      "Epoch [83/200], Step [1800/1800], Loss: 0.261767\n",
      "Epoch [84/200], Step [600/1800], Loss: 0.448642\n",
      "Epoch [84/200], Step [1200/1800], Loss: 0.403718\n",
      "Epoch [84/200], Step [1800/1800], Loss: 0.327127\n",
      "Epoch [85/200], Step [600/1800], Loss: 0.259534\n",
      "Epoch [85/200], Step [1200/1800], Loss: 0.553754\n",
      "Epoch [85/200], Step [1800/1800], Loss: 0.251381\n",
      "Epoch [86/200], Step [600/1800], Loss: 0.420424\n",
      "Epoch [86/200], Step [1200/1800], Loss: 0.285337\n",
      "Epoch [86/200], Step [1800/1800], Loss: 0.410277\n",
      "Epoch [87/200], Step [600/1800], Loss: 0.402043\n",
      "Epoch [87/200], Step [1200/1800], Loss: 0.443695\n",
      "Epoch [87/200], Step [1800/1800], Loss: 0.310155\n",
      "Epoch [88/200], Step [600/1800], Loss: 0.428489\n",
      "Epoch [88/200], Step [1200/1800], Loss: 0.215707\n",
      "Epoch [88/200], Step [1800/1800], Loss: 0.292462\n",
      "Epoch [89/200], Step [600/1800], Loss: 0.364354\n",
      "Epoch [89/200], Step [1200/1800], Loss: 0.241972\n",
      "Epoch [89/200], Step [1800/1800], Loss: 0.280130\n",
      "Epoch [90/200], Step [600/1800], Loss: 0.259830\n",
      "Epoch [90/200], Step [1200/1800], Loss: 0.288623\n",
      "Epoch [90/200], Step [1800/1800], Loss: 0.419791\n",
      "Epoch [91/200], Step [600/1800], Loss: 0.455098\n",
      "Epoch [91/200], Step [1200/1800], Loss: 0.469825\n",
      "Epoch [91/200], Step [1800/1800], Loss: 0.243687\n",
      "Epoch [92/200], Step [600/1800], Loss: 0.363679\n",
      "Epoch [92/200], Step [1200/1800], Loss: 0.300993\n",
      "Epoch [92/200], Step [1800/1800], Loss: 0.231297\n",
      "Epoch [93/200], Step [600/1800], Loss: 0.286002\n",
      "Epoch [93/200], Step [1200/1800], Loss: 0.468233\n",
      "Epoch [93/200], Step [1800/1800], Loss: 0.328381\n",
      "Epoch [94/200], Step [600/1800], Loss: 0.246122\n",
      "Epoch [94/200], Step [1200/1800], Loss: 0.280712\n",
      "Epoch [94/200], Step [1800/1800], Loss: 0.349569\n",
      "Epoch [95/200], Step [600/1800], Loss: 0.439483\n",
      "Epoch [95/200], Step [1200/1800], Loss: 0.353092\n",
      "Epoch [95/200], Step [1800/1800], Loss: 0.375015\n",
      "Epoch [96/200], Step [600/1800], Loss: 0.368312\n",
      "Epoch [96/200], Step [1200/1800], Loss: 0.606259\n",
      "Epoch [96/200], Step [1800/1800], Loss: 0.492010\n",
      "Epoch [97/200], Step [600/1800], Loss: 0.381995\n",
      "Epoch [97/200], Step [1200/1800], Loss: 0.278051\n",
      "Epoch [97/200], Step [1800/1800], Loss: 0.204590\n",
      "Epoch [98/200], Step [600/1800], Loss: 0.470255\n",
      "Epoch [98/200], Step [1200/1800], Loss: 0.481737\n",
      "Epoch [98/200], Step [1800/1800], Loss: 0.175634\n",
      "Epoch [99/200], Step [600/1800], Loss: 0.514088\n",
      "Epoch [99/200], Step [1200/1800], Loss: 0.379933\n",
      "Epoch [99/200], Step [1800/1800], Loss: 0.443663\n",
      "Epoch [100/200], Step [600/1800], Loss: 0.430144\n",
      "Epoch [100/200], Step [1200/1800], Loss: 0.362540\n",
      "Epoch [100/200], Step [1800/1800], Loss: 0.389342\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [114], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\eeg\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\git\\EEG-emotion\\methods\\Saliency_Emotion_EEG\\Models_DEAP.py:419\u001b[0m, in \u001b[0;36mBiHDM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    416\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-12\u001b[39m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# Vertical\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m x_vl, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRNN_VL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVL_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m x_vr, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mRNN_VL(x[:, VR_id]\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m), h0)\n\u001b[0;32m    422\u001b[0m x_v, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mRNN_V(x_vr \u001b[38;5;241m-\u001b[39m x_vl, h0)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\eeg\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\eeg\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:471\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN_TANH\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 471\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn_tanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    475\u001b[0m         result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mrnn_relu(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    476\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m    477\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not skip_training:\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, labels) in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "\n",
    "            data = data.to(device).float().permute(0, 2, 1)\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            outputs = model(data)    \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "\n",
    "            if (i+1) % 600 == 0:\n",
    "                    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.6f}')\n",
    "\n",
    "            if epoch > 5 and loss.item() < min_loss:\n",
    "                    min_loss = loss.item()\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'min_loss': min_loss\n",
    "                    }, model_path)\n",
    "                    print(f'Saved checkpoint - loss: {min_loss:.6f}')\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2b690d",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "49d5900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4ae100dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_n_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0fd919fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 44.48%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()  \n",
    "    if load_model:\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "\n",
    "    for data, labels in test_dataloader:\n",
    "        data = data.to(device).float().permute(0, 2, 1)\n",
    "        labels = labels.to(device).float().reshape(-1,)\n",
    "\n",
    "        outputs = model(data)\n",
    "\n",
    "        # torch.max returns (value, index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "acc = n_correct / n_samples * 100.0 \n",
    "print(f'Accuracy of the network: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b68160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
