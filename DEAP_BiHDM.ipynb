{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6430c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a97d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d90fb91",
   "metadata": {},
   "source": [
    "## DE Features (one subject)\n",
    "\n",
    "https://github.com/ynulonger/DE_CNN\n",
    "\n",
    "https://www.researchgate.net/publication/328504085_Continuous_Convolutional_Neural_Network_with_3D_Input_for_EEG-Based_Emotion_Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1894301",
   "metadata": {},
   "outputs": [],
   "source": [
    "deap_de_path = '../../methods/DE_CNN/1D_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a7a50f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__header__\n",
      "__version__\n",
      "__globals__\n",
      "base_data\n",
      "data\n",
      "valence_labels\n",
      "arousal_labels\n"
     ]
    }
   ],
   "source": [
    "s0 = scipy.io.loadmat(deap_de_path + 'DE_s01.mat')\n",
    "for i, key in enumerate(s0):\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ddea8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = s0['data']\n",
    "y_0_valence = s0['valence_labels']\n",
    "y_0_arousal = s0['arousal_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6d0a295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 4, 32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dfb76c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_0_valence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7756617a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(y_0_valence).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c6de5",
   "metadata": {},
   "source": [
    "### Merge all subjects' features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b64abb",
   "metadata": {},
   "source": [
    "Subjects number and indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38f48ed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 2399\n",
      "2 2400 4799\n",
      "3 4800 7199\n",
      "4 7200 9599\n",
      "5 9600 11999\n",
      "6 12000 14399\n",
      "7 14400 16799\n",
      "8 16800 19199\n",
      "9 19200 21599\n",
      "10 21600 23999\n",
      "11 24000 26399\n",
      "12 26400 28799\n",
      "13 28800 31199\n",
      "14 31200 33599\n",
      "15 33600 35999\n",
      "16 36000 38399\n",
      "17 38400 40799\n",
      "18 40800 43199\n",
      "19 43200 45599\n",
      "20 45600 47999\n",
      "21 48000 50399\n",
      "22 50400 52799\n",
      "23 52800 55199\n",
      "24 55200 57599\n",
      "25 57600 59999\n",
      "26 60000 62399\n",
      "27 62400 64799\n",
      "28 64800 67199\n",
      "29 67200 69599\n",
      "30 69600 71999\n",
      "31 72000 74399\n",
      "32 74400 76799\n"
     ]
    }
   ],
   "source": [
    "c = 2400\n",
    "\n",
    "for idx in range(32):\n",
    "    print(idx+1, idx*c, (idx+1)*c-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b96ea15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../methods/DE_CNN/1D_dataset/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deap_de_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5a5f1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_de_cnn_features = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd55a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_de_cnn_features:\n",
    "    de_cnn_features = np.empty((2400 * 32, 4, 32))\n",
    "    de_cnn_y_valence = np.empty((2400 * 32, 1))\n",
    "    de_cnn_y_arousal = np.empty((2400 * 32, 1))\n",
    "    \n",
    "    for i in range(1, 33):  # Subjects 1-32 in DEAP\n",
    "        subj_data = scipy.io.loadmat(deap_de_path + f'DE_s{i:02}.mat')\n",
    "\n",
    "        Xi_de = subj_data['data']\n",
    "        yi_valence = np.transpose(subj_data['valence_labels'])\n",
    "        yi_arousal = np.transpose(subj_data['arousal_labels'])\n",
    "        \n",
    "        idx = i-1  # indexing 0-31 for arrays\n",
    "        c = 2400  # size of each subject's trials*1s_windows\n",
    "\n",
    "        # efficient assigning, not really needed, could use np.append\n",
    "        de_cnn_features[idx*c:(idx+1)*c] = Xi_de\n",
    "        de_cnn_y_valence[idx*c:(idx+1)*c] = yi_valence\n",
    "        de_cnn_y_arousal[idx*c:(idx+1)*c] = yi_arousal\n",
    "        \n",
    "        save_dict = {'data': de_cnn_features, \n",
    "                     'valence_labels': de_cnn_y_valence, \n",
    "                     'arousal_labels': de_cnn_y_arousal}\n",
    "         \n",
    "    np.save(deap_de_path + 'DE_merged.npy', save_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcfb2001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from file.\n",
      "(76800, 4, 32)\n",
      "(76800, 1)\n",
      "(76800, 1)\n"
     ]
    }
   ],
   "source": [
    "if not merge_de_cnn_features:\n",
    "    de_cnn_merged = np.load(deap_de_path + 'DE_merged.npy', allow_pickle=True).item()\n",
    "    de_cnn_features = de_cnn_merged['data']\n",
    "    de_cnn_y_valence = de_cnn_merged['valence_labels']\n",
    "    de_cnn_y_arousal = de_cnn_merged['arousal_labels']\n",
    "    \n",
    "    print('Loaded from file.')\n",
    "    print(de_cnn_features.shape)\n",
    "    print(de_cnn_y_valence.shape)\n",
    "    print(de_cnn_y_arousal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0af6c0",
   "metadata": {},
   "source": [
    "## Load DE Features (all subjects)\n",
    "\n",
    "https://github.com/gzoumpourlis/DEAP_MNE_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "309919d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_features_path = '../../preprocessing/DEAP_MNE_preprocessing/features_new/de_feats_merged.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "529ded37",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_features = np.load(de_features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de730857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 32, 5, 232)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2dbdd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "deap_path = '../../datasets/DEAP/merged/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "798fae6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.load(deap_path + 'deap_full_labels.npy')\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391535f0",
   "metadata": {},
   "source": [
    "Column 0 is Valence, 1 is Arousal, 2 is quadrants notation (HAHV, HALV, LAHV, LALV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1973329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valence = 0\n",
    "arousal = 1\n",
    "quadrants = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f62053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y[:, valence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09519b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f63235",
   "metadata": {},
   "source": [
    "## Define DEAP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d279b18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T23:18:14.892936Z",
     "start_time": "2022-11-16T23:18:14.886230Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44850796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T23:18:14.903641Z",
     "start_time": "2022-11-16T23:18:14.895007Z"
    }
   },
   "outputs": [],
   "source": [
    "class DEAPDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.X = data\n",
    "        self.y = labels\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed4d756",
   "metadata": {},
   "source": [
    "Either use 'de_features' (DEAP_MNE_preprocessing) or 'de_cnn_features' (DE_CNN)\n",
    "\n",
    "Labels are 'y' or 'de_cnn_y_valence/de_cnn_y_arousal' respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13655227",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_de_cnn = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1977f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_de_cnn:\n",
    "    deap_dataset = DEAPDataset(de_cnn_features, de_cnn_y_valence)\n",
    "else:\n",
    "    deap_dataset = DEAPDataset(de_features, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26d9426",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "caf16a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99e08902",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b00e2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(deap_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76c26f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76800"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader) * batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e1a57c",
   "metadata": {},
   "source": [
    "## Define BiHDM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c4014e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb6597ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models_DEAP import BiHDM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1084561d",
   "metadata": {},
   "source": [
    "### BiHDM Initialization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b784f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size=32\n",
    "num_layers=2\n",
    "input_size=4\n",
    "n_classes=1\n",
    "\n",
    "# batch_first=False\n",
    "# bidirectional=False\n",
    "\n",
    "fc_input=448\n",
    "fc_hidden=96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe6a55ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiHDM(hidden_size=hidden_size, num_layers=num_layers, input_size=input_size, \n",
    "              fc_input=fc_input, fc_hidden=fc_hidden, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a128b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiHDM(\n",
       "  (RNN_VL): RNN(4, 32, num_layers=2)\n",
       "  (RNN_VR): RNN(4, 32, num_layers=2)\n",
       "  (RNN_V): RNN(32, 32, num_layers=2)\n",
       "  (RNN_HL): RNN(4, 32, num_layers=2)\n",
       "  (RNN_HR): RNN(4, 32, num_layers=2)\n",
       "  (RNN_H): RNN(32, 32, num_layers=2)\n",
       "  (fc_v): Sequential(\n",
       "    (0): Linear(in_features=448, out_features=96, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc_h): Sequential(\n",
       "    (0): Linear(in_features=448, out_features=96, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc_c): Sequential(\n",
       "    (0): Linear(in_features=96, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81909eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 32, 5, 232)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = de_features[:64] # small batch deap_mne_preprocessing\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fafc0d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 4, 32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = de_cnn_features[:64] # small batch de_cnn\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c22c788",
   "metadata": {},
   "source": [
    "## Training BiHDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55cf0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "282fd287",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "betas=(0.9, 0.999)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77240e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7dbbcebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0, loss: 0.5709908604621887\n",
      "batch: 1, loss: 0.5710475444793701\n",
      "batch: 2, loss: 0.5710583329200745\n",
      "batch: 3, loss: 0.5710269212722778\n",
      "batch: 4, loss: 0.5709583759307861\n",
      "batch: 5, loss: 0.6689745187759399\n",
      "batch: 6, loss: 0.8326308727264404\n",
      "batch: 7, loss: 0.7017001509666443\n",
      "batch: 8, loss: 0.5706980228424072\n",
      "batch: 9, loss: 0.5706344246864319\n",
      "batch: 10, loss: 0.570536732673645\n",
      "batch: 11, loss: 0.5704078078269958\n",
      "batch: 12, loss: 0.570252001285553\n",
      "batch: 13, loss: 0.570070743560791\n",
      "batch: 14, loss: 0.5698675513267517\n",
      "batch: 15, loss: 0.83408123254776\n",
      "batch: 16, loss: 0.8342747688293457\n",
      "batch: 17, loss: 0.8343794941902161\n",
      "batch: 18, loss: 0.834405779838562\n",
      "batch: 19, loss: 0.8343607187271118\n",
      "batch: 20, loss: 0.8342517614364624\n",
      "batch: 21, loss: 0.8340850472450256\n",
      "batch: 22, loss: 0.8338665962219238\n",
      "batch: 23, loss: 0.8336013555526733\n",
      "batch: 24, loss: 0.8332943320274353\n",
      "batch: 25, loss: 0.8329495191574097\n",
      "batch: 26, loss: 0.8325717449188232\n",
      "batch: 27, loss: 0.8321627378463745\n",
      "batch: 28, loss: 0.6039888858795166\n",
      "batch: 29, loss: 0.5717281699180603\n",
      "batch: 30, loss: 0.8311064839363098\n",
      "batch: 31, loss: 0.7984702587127686\n",
      "batch: 32, loss: 0.5724265575408936\n",
      "batch: 33, loss: 0.5726152658462524\n",
      "batch: 34, loss: 0.5727444291114807\n",
      "batch: 35, loss: 0.5728201866149902\n",
      "batch: 36, loss: 0.5728476643562317\n",
      "batch: 37, loss: 0.5728318095207214\n",
      "batch: 38, loss: 0.5727769136428833\n",
      "batch: 39, loss: 0.5726866722106934\n",
      "batch: 40, loss: 0.572564959526062\n",
      "batch: 41, loss: 0.5724146366119385\n",
      "batch: 42, loss: 0.5722388029098511\n",
      "batch: 43, loss: 0.5720394849777222\n",
      "batch: 44, loss: 0.5718205571174622\n",
      "batch: 45, loss: 0.5715823173522949\n",
      "batch: 46, loss: 0.5713270306587219\n",
      "batch: 47, loss: 0.5710564255714417\n",
      "batch: 48, loss: 0.570774495601654\n",
      "batch: 49, loss: 0.5704787969589233\n",
      "batch: 50, loss: 0.6688804626464844\n",
      "batch: 51, loss: 0.833759069442749\n",
      "batch: 52, loss: 0.8340200185775757\n",
      "batch: 53, loss: 0.8341860175132751\n",
      "batch: 54, loss: 0.8342670798301697\n",
      "batch: 55, loss: 0.8342721462249756\n",
      "batch: 56, loss: 0.8342069983482361\n",
      "batch: 57, loss: 0.834080696105957\n",
      "batch: 58, loss: 0.8338985443115234\n",
      "batch: 59, loss: 0.8336654305458069\n",
      "batch: 60, loss: 0.83338862657547\n",
      "batch: 61, loss: 0.8330708742141724\n",
      "batch: 62, loss: 0.8327165842056274\n",
      "batch: 63, loss: 0.8323298096656799\n",
      "batch: 64, loss: 0.8319136500358582\n",
      "batch: 65, loss: 0.8314716815948486\n",
      "batch: 66, loss: 0.8310052156448364\n",
      "batch: 67, loss: 0.8305190205574036\n",
      "batch: 68, loss: 0.8300122022628784\n",
      "batch: 69, loss: 0.8294896483421326\n",
      "batch: 70, loss: 0.8289510011672974\n",
      "batch: 71, loss: 0.8283993005752563\n",
      "batch: 72, loss: 0.8278358578681946\n",
      "batch: 73, loss: 0.6064551472663879\n",
      "batch: 74, loss: 0.5752845406532288\n",
      "batch: 75, loss: 0.5755817890167236\n",
      "batch: 76, loss: 0.5758057832717896\n",
      "batch: 77, loss: 0.5759683847427368\n",
      "batch: 78, loss: 0.5760728120803833\n",
      "batch: 79, loss: 0.5761255621910095\n",
      "batch: 80, loss: 0.5761324167251587\n",
      "batch: 81, loss: 0.5760973691940308\n",
      "batch: 82, loss: 0.5760242938995361\n",
      "batch: 83, loss: 0.5759180784225464\n",
      "batch: 84, loss: 0.7322566509246826\n",
      "batch: 85, loss: 0.8262764811515808\n",
      "batch: 86, loss: 0.8263306617736816\n",
      "batch: 87, loss: 0.8263117074966431\n",
      "batch: 88, loss: 0.6070301532745361\n",
      "batch: 89, loss: 0.575745701789856\n",
      "batch: 90, loss: 0.5757315158843994\n",
      "batch: 91, loss: 0.6070027351379395\n",
      "batch: 92, loss: 0.8263751268386841\n",
      "batch: 93, loss: 0.7636932730674744\n",
      "batch: 94, loss: 0.5755976438522339\n",
      "batch: 95, loss: 0.5755689144134521\n",
      "batch: 96, loss: 0.5755026936531067\n",
      "batch: 97, loss: 0.5754019618034363\n",
      "batch: 98, loss: 0.5752701759338379\n",
      "batch: 99, loss: 0.5751110315322876\n",
      "batch: 100, loss: 0.5749270915985107\n",
      "batch: 101, loss: 0.5747201442718506\n",
      "batch: 102, loss: 0.574493408203125\n",
      "batch: 103, loss: 0.5742491483688354\n",
      "batch: 104, loss: 0.5739884376525879\n",
      "batch: 105, loss: 0.5737125873565674\n",
      "batch: 106, loss: 0.5734242796897888\n",
      "batch: 107, loss: 0.5731239318847656\n",
      "batch: 108, loss: 0.5728132128715515\n",
      "batch: 109, loss: 0.572492778301239\n",
      "batch: 110, loss: 0.5721640586853027\n",
      "batch: 111, loss: 0.5718278288841248\n",
      "batch: 112, loss: 0.7015867829322815\n",
      "batch: 113, loss: 0.8320820331573486\n",
      "batch: 114, loss: 0.8323678374290466\n",
      "batch: 115, loss: 0.832556962966919\n",
      "batch: 116, loss: 0.832656979560852\n",
      "batch: 117, loss: 0.8326801061630249\n",
      "batch: 118, loss: 0.6034920811653137\n",
      "batch: 119, loss: 0.5707614421844482\n",
      "batch: 120, loss: 0.5707250237464905\n",
      "batch: 121, loss: 0.5706515312194824\n",
      "batch: 122, loss: 0.570544958114624\n",
      "batch: 123, loss: 0.636077880859375\n",
      "batch: 124, loss: 0.8332681655883789\n",
      "batch: 125, loss: 0.8333634734153748\n",
      "batch: 126, loss: 0.8333810567855835\n",
      "batch: 127, loss: 0.833328366279602\n",
      "batch: 128, loss: 0.8332133293151855\n",
      "batch: 129, loss: 0.8330410122871399\n",
      "batch: 130, loss: 0.8328179121017456\n",
      "batch: 131, loss: 0.8325487971305847\n",
      "batch: 132, loss: 0.8322386145591736\n",
      "batch: 133, loss: 0.8318915367126465\n",
      "batch: 134, loss: 0.8315115571022034\n",
      "batch: 135, loss: 0.831102192401886\n",
      "batch: 136, loss: 0.830665111541748\n",
      "batch: 137, loss: 0.8302044868469238\n",
      "batch: 138, loss: 0.82972252368927\n",
      "batch: 139, loss: 0.8292214274406433\n",
      "batch: 140, loss: 0.733111560344696\n",
      "batch: 141, loss: 0.5741692185401917\n",
      "batch: 142, loss: 0.7011493444442749\n",
      "batch: 143, loss: 0.827468991279602\n",
      "batch: 144, loss: 0.8270807266235352\n",
      "batch: 145, loss: 0.8266639709472656\n",
      "batch: 146, loss: 0.8262215852737427\n",
      "batch: 147, loss: 0.8257566690444946\n",
      "batch: 148, loss: 0.6075618863105774\n",
      "batch: 149, loss: 0.5767720341682434\n",
      "batch: 150, loss: 0.5770184397697449\n",
      "batch: 151, loss: 0.5771931409835815\n",
      "batch: 152, loss: 0.5773094892501831\n",
      "batch: 153, loss: 0.5773675441741943\n",
      "batch: 154, loss: 0.5773892402648926\n",
      "batch: 155, loss: 0.6698924899101257\n",
      "batch: 156, loss: 0.8241567611694336\n",
      "batch: 157, loss: 0.7007405757904053\n",
      "batch: 158, loss: 0.5773828029632568\n",
      "batch: 159, loss: 0.5773677229881287\n",
      "batch: 160, loss: 0.5773133635520935\n",
      "batch: 161, loss: 0.5772233605384827\n",
      "batch: 162, loss: 0.5771012902259827\n",
      "batch: 163, loss: 0.5769504904747009\n",
      "batch: 164, loss: 0.5767738819122314\n",
      "batch: 165, loss: 0.5765738487243652\n",
      "batch: 166, loss: 0.6074848771095276\n",
      "batch: 167, loss: 0.8257013559341431\n",
      "batch: 168, loss: 0.763416051864624\n",
      "batch: 169, loss: 0.5758639574050903\n",
      "batch: 170, loss: 0.5757256150245667\n",
      "batch: 171, loss: 0.5755603909492493\n",
      "batch: 172, loss: 0.575370728969574\n",
      "batch: 173, loss: 0.5751591324806213\n",
      "batch: 174, loss: 0.5749278664588928\n",
      "batch: 175, loss: 0.5746790766716003\n",
      "batch: 176, loss: 0.5744143724441528\n",
      "batch: 177, loss: 0.5741356611251831\n",
      "batch: 178, loss: 0.5738438963890076\n",
      "batch: 179, loss: 0.5735407471656799\n",
      "batch: 180, loss: 0.8294325470924377\n",
      "batch: 181, loss: 0.7976377010345459\n",
      "batch: 182, loss: 0.5728325247764587\n",
      "batch: 183, loss: 0.5726428031921387\n",
      "batch: 184, loss: 0.5724316239356995\n",
      "batch: 185, loss: 0.5722010731697083\n",
      "batch: 186, loss: 0.5719530582427979\n",
      "batch: 187, loss: 0.7015562057495117\n",
      "batch: 188, loss: 0.8317232131958008\n",
      "batch: 189, loss: 0.8319251537322998\n",
      "batch: 190, loss: 0.8320386409759521\n",
      "batch: 191, loss: 0.8320726156234741\n",
      "batch: 192, loss: 0.8320350646972656\n",
      "batch: 193, loss: 0.8319332599639893\n",
      "batch: 194, loss: 0.8317735195159912\n",
      "batch: 195, loss: 0.831561803817749\n",
      "batch: 196, loss: 0.8313033580780029\n",
      "batch: 197, loss: 0.8310027718544006\n",
      "batch: 198, loss: 0.7660671472549438\n",
      "batch: 199, loss: 0.5725392699241638\n",
      "batch: 200, loss: 0.5727367401123047\n",
      "batch: 201, loss: 0.572874128818512\n",
      "batch: 202, loss: 0.7013695240020752\n",
      "batch: 203, loss: 0.8296776413917542\n",
      "batch: 204, loss: 0.8295160531997681\n",
      "batch: 205, loss: 0.8293027877807617\n",
      "batch: 206, loss: 0.8290432691574097\n",
      "batch: 207, loss: 0.8287419080734253\n",
      "batch: 208, loss: 0.605821967124939\n",
      "batch: 209, loss: 0.5742320418357849\n",
      "batch: 210, loss: 0.8279478549957275\n",
      "batch: 211, loss: 0.8277111053466797\n",
      "batch: 212, loss: 0.8274301886558533\n",
      "batch: 213, loss: 0.8271105289459229\n",
      "batch: 214, loss: 0.826755166053772\n",
      "batch: 215, loss: 0.732332170009613\n",
      "batch: 216, loss: 0.575894296169281\n",
      "batch: 217, loss: 0.7009148001670837\n",
      "batch: 218, loss: 0.8254556059837341\n",
      "batch: 219, loss: 0.8251538276672363\n",
      "batch: 220, loss: 0.824815034866333\n",
      "batch: 221, loss: 0.8244431018829346\n",
      "batch: 222, loss: 0.824041485786438\n",
      "batch: 223, loss: 0.6084872484207153\n",
      "batch: 224, loss: 0.5780273079872131\n",
      "batch: 225, loss: 0.5782313346862793\n",
      "batch: 226, loss: 0.6089299917221069\n",
      "batch: 227, loss: 0.8226956725120544\n",
      "batch: 228, loss: 0.7615398168563843\n",
      "batch: 229, loss: 0.57877117395401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 230, loss: 0.5788710117340088\n",
      "batch: 231, loss: 0.5789196491241455\n",
      "batch: 232, loss: 0.7005223035812378\n",
      "batch: 233, loss: 0.8221120834350586\n",
      "batch: 234, loss: 0.6701322793960571\n",
      "batch: 235, loss: 0.5790378451347351\n",
      "batch: 236, loss: 0.7612394690513611\n",
      "batch: 237, loss: 0.8219336867332458\n",
      "batch: 238, loss: 0.6094856858253479\n",
      "batch: 239, loss: 0.5791931748390198\n",
      "batch: 240, loss: 0.5791904330253601\n",
      "batch: 241, loss: 0.6094828248023987\n",
      "batch: 242, loss: 0.8219242691993713\n",
      "batch: 243, loss: 0.7612193822860718\n",
      "batch: 244, loss: 0.5790886878967285\n",
      "batch: 245, loss: 0.5790659189224243\n",
      "batch: 246, loss: 0.5790042877197266\n",
      "batch: 247, loss: 0.7005243897438049\n",
      "batch: 248, loss: 0.8222451210021973\n",
      "batch: 249, loss: 0.6701051592826843\n",
      "batch: 250, loss: 0.5787806510925293\n",
      "batch: 251, loss: 0.5787171721458435\n",
      "batch: 252, loss: 0.5786188244819641\n",
      "batch: 253, loss: 0.792151927947998\n",
      "batch: 254, loss: 0.8227721452713013\n",
      "batch: 255, loss: 0.5783973336219788\n",
      "batch: 256, loss: 0.6089072227478027\n",
      "batch: 257, loss: 0.8229638338088989\n",
      "batch: 258, loss: 0.7618018388748169\n",
      "batch: 259, loss: 0.5782565474510193\n",
      "batch: 260, loss: 0.5782263875007629\n",
      "batch: 261, loss: 0.5781580209732056\n",
      "batch: 262, loss: 0.7006421685218811\n",
      "batch: 263, loss: 0.8233399987220764\n",
      "batch: 264, loss: 0.8233724236488342\n",
      "batch: 265, loss: 0.823334276676178\n",
      "batch: 266, loss: 0.8232325911521912\n",
      "batch: 267, loss: 0.8230738639831543\n",
      "batch: 268, loss: 0.8228638172149658\n",
      "batch: 269, loss: 0.8226075768470764\n",
      "batch: 270, loss: 0.57877516746521\n",
      "batch: 271, loss: 0.5789438486099243\n",
      "batch: 272, loss: 0.5790544152259827\n",
      "batch: 273, loss: 0.5791127681732178\n",
      "batch: 274, loss: 0.5791239738464355\n",
      "batch: 275, loss: 0.6701472997665405\n",
      "batch: 276, loss: 0.8219485878944397\n",
      "batch: 277, loss: 0.8219208717346191\n",
      "batch: 278, loss: 0.8218287229537964\n",
      "batch: 279, loss: 0.8216785788536072\n",
      "batch: 280, loss: 0.8214765787124634\n",
      "batch: 281, loss: 0.8212276697158813\n",
      "batch: 282, loss: 0.8209370374679565\n",
      "batch: 283, loss: 0.8206082582473755\n",
      "batch: 284, loss: 0.8202458024024963\n",
      "batch: 285, loss: 0.819852888584137\n",
      "batch: 286, loss: 0.8194327354431152\n",
      "batch: 287, loss: 0.8189879059791565\n",
      "batch: 288, loss: 0.8185213804244995\n",
      "batch: 289, loss: 0.8180350065231323\n",
      "batch: 290, loss: 0.8175309896469116\n",
      "batch: 291, loss: 0.8170109987258911\n",
      "batch: 292, loss: 0.8164772391319275\n",
      "batch: 293, loss: 0.8159304857254028\n",
      "batch: 294, loss: 0.8153725862503052\n",
      "batch: 295, loss: 0.8148049116134644\n",
      "batch: 296, loss: 0.8142279386520386\n",
      "batch: 297, loss: 0.8136429786682129\n",
      "batch: 298, loss: 0.8130508065223694\n",
      "batch: 299, loss: 0.812452495098114\n",
      "batch: 300, loss: 0.587052583694458\n",
      "batch: 301, loss: 0.5874448418617249\n",
      "batch: 302, loss: 0.5877559185028076\n",
      "batch: 303, loss: 0.6436634063720703\n",
      "batch: 304, loss: 0.8104262351989746\n",
      "batch: 305, loss: 0.7269948720932007\n",
      "batch: 306, loss: 0.5886425375938416\n",
      "batch: 307, loss: 0.6992322206497192\n",
      "batch: 308, loss: 0.8094783425331116\n",
      "batch: 309, loss: 0.6716772317886353\n",
      "batch: 310, loss: 0.5892966985702515\n",
      "batch: 311, loss: 0.5893993377685547\n",
      "batch: 312, loss: 0.589449405670166\n",
      "batch: 313, loss: 0.5894522666931152\n",
      "batch: 314, loss: 0.58941251039505\n",
      "batch: 315, loss: 0.5893344283103943\n",
      "batch: 316, loss: 0.6167116165161133\n",
      "batch: 317, loss: 0.8093032240867615\n",
      "batch: 318, loss: 0.7542949318885803\n",
      "batch: 319, loss: 0.5889940857887268\n",
      "batch: 320, loss: 0.5889246463775635\n",
      "batch: 321, loss: 0.5888198018074036\n",
      "batch: 322, loss: 0.699246883392334\n",
      "batch: 323, loss: 0.8099575042724609\n",
      "batch: 324, loss: 0.6715792417526245\n",
      "batch: 325, loss: 0.5884577035903931\n",
      "batch: 326, loss: 0.5883664488792419\n",
      "batch: 327, loss: 0.5882419943809509\n",
      "batch: 328, loss: 0.588087797164917\n",
      "batch: 329, loss: 0.5879068374633789\n",
      "batch: 330, loss: 0.5877018570899963\n",
      "batch: 331, loss: 0.5874752998352051\n",
      "batch: 332, loss: 0.5872292518615723\n",
      "batch: 333, loss: 0.5869656801223755\n",
      "batch: 334, loss: 0.5866866111755371\n",
      "batch: 335, loss: 0.5863933563232422\n",
      "batch: 336, loss: 0.5860874652862549\n",
      "batch: 337, loss: 0.6996133327484131\n",
      "batch: 338, loss: 0.8138083219528198\n",
      "batch: 339, loss: 0.6710788011550903\n",
      "batch: 340, loss: 0.5851050019264221\n",
      "batch: 341, loss: 0.7571405172348022\n",
      "batch: 342, loss: 0.8147557973861694\n",
      "batch: 343, loss: 0.8148695826530457\n",
      "batch: 344, loss: 0.8149057030677795\n",
      "batch: 345, loss: 0.5846445560455322\n",
      "batch: 346, loss: 0.5846269130706787\n",
      "batch: 347, loss: 0.5845691561698914\n",
      "batch: 348, loss: 0.5844752788543701\n",
      "batch: 349, loss: 0.5843490362167358\n",
      "batch: 350, loss: 0.6709108948707581\n",
      "batch: 351, loss: 0.8156245350837708\n",
      "batch: 352, loss: 0.8157240748405457\n",
      "batch: 353, loss: 0.8157471418380737\n",
      "batch: 354, loss: 0.8157014846801758\n",
      "batch: 355, loss: 0.8155938386917114\n",
      "batch: 356, loss: 0.6420082449913025\n",
      "batch: 357, loss: 0.5842991471290588\n",
      "batch: 358, loss: 0.786385178565979\n",
      "batch: 359, loss: 0.8151435256004333\n",
      "batch: 360, loss: 0.5845561623573303\n",
      "batch: 361, loss: 0.5846289992332458\n",
      "batch: 362, loss: 0.5846526026725769\n",
      "batch: 363, loss: 0.6421958804130554\n",
      "batch: 364, loss: 0.8149338364601135\n",
      "batch: 365, loss: 0.8149092793464661\n",
      "batch: 366, loss: 0.8148207068443298\n",
      "batch: 367, loss: 0.8146744966506958\n",
      "batch: 368, loss: 0.8144768476486206\n",
      "batch: 369, loss: 0.8142327070236206\n",
      "batch: 370, loss: 0.8139466643333435\n",
      "batch: 371, loss: 0.813623309135437\n",
      "batch: 372, loss: 0.8132659196853638\n",
      "batch: 373, loss: 0.6145615577697754\n",
      "batch: 374, loss: 0.5864784717559814\n",
      "batch: 375, loss: 0.5866594910621643\n",
      "batch: 376, loss: 0.5867806077003479\n",
      "batch: 377, loss: 0.5868472456932068\n",
      "batch: 378, loss: 0.5868653059005737\n",
      "batch: 379, loss: 0.5868394374847412\n",
      "batch: 380, loss: 0.5867742300033569\n",
      "batch: 381, loss: 0.5866732597351074\n",
      "batch: 382, loss: 0.5865404605865479\n",
      "batch: 383, loss: 0.5863789319992065\n",
      "batch: 384, loss: 0.5861912965774536\n",
      "batch: 385, loss: 0.5859806537628174\n",
      "batch: 386, loss: 0.5857487916946411\n",
      "batch: 387, loss: 0.585498571395874\n",
      "batch: 388, loss: 0.585231602191925\n",
      "batch: 389, loss: 0.584949254989624\n",
      "batch: 390, loss: 0.5846532583236694\n",
      "batch: 391, loss: 0.584345281124115\n",
      "batch: 392, loss: 0.5840262770652771\n",
      "batch: 393, loss: 0.6417895555496216\n",
      "batch: 394, loss: 0.8164619207382202\n",
      "batch: 395, loss: 0.7291526794433594\n",
      "batch: 396, loss: 0.5829638242721558\n",
      "batch: 397, loss: 0.5827513337135315\n",
      "batch: 398, loss: 0.5825188159942627\n",
      "batch: 399, loss: 0.5822675824165344\n",
      "batch: 400, loss: 0.5820001363754272\n",
      "batch: 401, loss: 0.581717312335968\n",
      "batch: 402, loss: 0.5814220905303955\n",
      "batch: 403, loss: 0.5811145305633545\n",
      "batch: 404, loss: 0.5807961225509644\n",
      "batch: 405, loss: 0.5804682374000549\n",
      "batch: 406, loss: 0.5801317691802979\n",
      "batch: 407, loss: 0.5797876119613647\n",
      "batch: 408, loss: 0.5794367790222168\n",
      "batch: 409, loss: 0.5790793299674988\n",
      "batch: 410, loss: 0.5787168145179749\n",
      "batch: 411, loss: 0.5783490538597107\n",
      "batch: 412, loss: 0.5779775381088257\n",
      "batch: 413, loss: 0.5776017308235168\n",
      "batch: 414, loss: 0.5772224068641663\n",
      "batch: 415, loss: 0.5768396854400635\n",
      "batch: 416, loss: 0.7630720734596252\n",
      "batch: 417, loss: 0.8256852626800537\n",
      "batch: 418, loss: 0.8259838223457336\n",
      "batch: 419, loss: 0.8261852264404297\n",
      "batch: 420, loss: 0.5756595134735107\n",
      "batch: 421, loss: 0.5755394697189331\n",
      "batch: 422, loss: 0.5753899216651917\n",
      "batch: 423, loss: 0.5752145051956177\n",
      "batch: 424, loss: 0.5750160813331604\n",
      "batch: 425, loss: 0.5747958421707153\n",
      "batch: 426, loss: 0.5745570659637451\n",
      "batch: 427, loss: 0.7011738419532776\n",
      "batch: 428, loss: 0.828335165977478\n",
      "batch: 429, loss: 0.6694032549858093\n",
      "batch: 430, loss: 0.5737884044647217\n",
      "batch: 431, loss: 0.7650982141494751\n",
      "batch: 432, loss: 0.8290796279907227\n",
      "batch: 433, loss: 0.6054081320762634\n",
      "batch: 434, loss: 0.5733655095100403\n",
      "batch: 435, loss: 0.5732530951499939\n",
      "batch: 436, loss: 0.6051701307296753\n",
      "batch: 437, loss: 0.8297847509384155\n",
      "batch: 438, loss: 0.8298990726470947\n",
      "batch: 439, loss: 0.8299341201782227\n",
      "batch: 440, loss: 0.7335114479064941\n",
      "batch: 441, loss: 0.5729112029075623\n",
      "batch: 442, loss: 0.7013766169548035\n",
      "batch: 443, loss: 0.8298375606536865\n",
      "batch: 444, loss: 0.8297649621963501\n",
      "batch: 445, loss: 0.8296312689781189\n",
      "batch: 446, loss: 0.6372761726379395\n",
      "batch: 447, loss: 0.5733345746994019\n",
      "batch: 448, loss: 0.7972371578216553\n",
      "batch: 449, loss: 0.829089343547821\n",
      "batch: 450, loss: 0.5736336708068848\n",
      "batch: 451, loss: 0.5737189054489136\n",
      "batch: 452, loss: 0.5737545490264893\n",
      "batch: 453, loss: 0.5737456679344177\n",
      "batch: 454, loss: 0.5736969113349915\n",
      "batch: 455, loss: 0.5736122131347656\n",
      "batch: 456, loss: 0.573495090007782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 457, loss: 0.5733487606048584\n",
      "batch: 458, loss: 0.5731762647628784\n",
      "batch: 459, loss: 0.5729803442955017\n",
      "batch: 460, loss: 0.5727632641792297\n",
      "batch: 461, loss: 0.5725269317626953\n",
      "batch: 462, loss: 0.5722736716270447\n",
      "batch: 463, loss: 0.5720051527023315\n",
      "batch: 464, loss: 0.5717227458953857\n",
      "batch: 465, loss: 0.5714279413223267\n",
      "batch: 466, loss: 0.5711219310760498\n",
      "batch: 467, loss: 0.5708063840866089\n",
      "batch: 468, loss: 0.570481538772583\n",
      "batch: 469, loss: 0.5701488852500916\n",
      "batch: 470, loss: 0.5698089003562927\n",
      "batch: 471, loss: 0.5694628357887268\n",
      "batch: 472, loss: 0.5691109299659729\n",
      "batch: 473, loss: 0.5687540173530579\n",
      "batch: 474, loss: 0.5683923363685608\n",
      "batch: 475, loss: 0.568026602268219\n",
      "batch: 476, loss: 0.5676573514938354\n",
      "batch: 477, loss: 0.5672848224639893\n",
      "batch: 478, loss: 0.80381178855896\n",
      "batch: 479, loss: 0.8380440473556519\n",
      "batch: 480, loss: 0.5663988590240479\n",
      "batch: 481, loss: 0.5661661028862\n",
      "batch: 482, loss: 0.5659165382385254\n",
      "batch: 483, loss: 0.5656516551971436\n",
      "batch: 484, loss: 0.5653735399246216\n",
      "batch: 485, loss: 0.5650833249092102\n",
      "batch: 486, loss: 0.5647817254066467\n",
      "batch: 487, loss: 0.7026659250259399\n",
      "batch: 488, loss: 0.8412216305732727\n",
      "batch: 489, loss: 0.8414768576622009\n",
      "batch: 490, loss: 0.8416369557380676\n",
      "batch: 491, loss: 0.8417113423347473\n",
      "batch: 492, loss: 0.8417090177536011\n",
      "batch: 493, loss: 0.8416374921798706\n",
      "batch: 494, loss: 0.8415032625198364\n",
      "batch: 495, loss: 0.8413134217262268\n",
      "batch: 496, loss: 0.8410729765892029\n",
      "batch: 497, loss: 0.8407874703407288\n",
      "batch: 498, loss: 0.7715393304824829\n",
      "batch: 499, loss: 0.5650266408920288\n",
      "batch: 500, loss: 0.6682144999504089\n",
      "batch: 501, loss: 0.839667022228241\n",
      "batch: 502, loss: 0.7024908065795898\n",
      "batch: 503, loss: 0.5657632350921631\n",
      "batch: 504, loss: 0.7365788817405701\n",
      "batch: 505, loss: 0.8388174772262573\n",
      "batch: 506, loss: 0.6342946290969849\n",
      "batch: 507, loss: 0.5663337707519531\n",
      "batch: 508, loss: 0.8043144941329956\n",
      "batch: 509, loss: 0.8381463289260864\n",
      "batch: 510, loss: 0.8379391431808472\n",
      "batch: 511, loss: 0.8376837372779846\n",
      "batch: 512, loss: 0.837384819984436\n",
      "batch: 513, loss: 0.769628643989563\n",
      "batch: 514, loss: 0.567635178565979\n",
      "batch: 515, loss: 0.5678305625915527\n",
      "batch: 516, loss: 0.567966103553772\n",
      "batch: 517, loss: 0.5680478811264038\n",
      "batch: 518, loss: 0.5680813193321228\n",
      "batch: 519, loss: 0.5680708885192871\n",
      "batch: 520, loss: 0.5680211782455444\n",
      "batch: 521, loss: 0.5679360032081604\n",
      "batch: 522, loss: 0.5678192377090454\n",
      "batch: 523, loss: 0.5676737427711487\n",
      "batch: 524, loss: 0.5675025582313538\n",
      "batch: 525, loss: 0.567305862903595\n",
      "batch: 526, loss: 0.5670908093452454\n",
      "batch: 527, loss: 0.56685870885849\n",
      "batch: 528, loss: 0.5666024684906006\n",
      "batch: 529, loss: 0.5663242936134338\n",
      "batch: 530, loss: 0.5660529732704163\n",
      "batch: 531, loss: 0.5657522678375244\n",
      "batch: 532, loss: 0.7025144696235657\n",
      "batch: 533, loss: 0.8399122357368469\n",
      "batch: 534, loss: 0.6681772470474243\n",
      "batch: 535, loss: 0.5648224353790283\n",
      "batch: 536, loss: 0.5646295547485352\n",
      "batch: 537, loss: 0.5644169449806213\n",
      "batch: 538, loss: 0.5641944408416748\n",
      "batch: 539, loss: 0.5639252066612244\n",
      "batch: 540, loss: 0.5636676549911499\n",
      "batch: 541, loss: 0.5982761979103088\n",
      "batch: 542, loss: 0.8427193760871887\n",
      "batch: 543, loss: 0.7729742527008057\n",
      "batch: 544, loss: 0.5627532601356506\n",
      "batch: 545, loss: 0.5625839829444885\n",
      "batch: 546, loss: 0.5623887777328491\n",
      "batch: 547, loss: 0.5621176362037659\n",
      "batch: 548, loss: 0.5618963241577148\n",
      "batch: 549, loss: 0.5616244077682495\n",
      "batch: 550, loss: 0.561425507068634\n",
      "batch: 551, loss: 0.5611276626586914\n",
      "batch: 552, loss: 0.5608322620391846\n",
      "batch: 553, loss: 0.8105324506759644\n",
      "batch: 554, loss: 0.8465709686279297\n",
      "batch: 555, loss: 0.5600051879882812\n",
      "batch: 556, loss: 0.5598545074462891\n",
      "batch: 557, loss: 0.5597730875015259\n",
      "batch: 558, loss: 0.5595431923866272\n",
      "batch: 559, loss: 0.5592852234840393\n",
      "batch: 560, loss: 0.5589932203292847\n",
      "batch: 561, loss: 0.5587252378463745\n",
      "batch: 562, loss: 0.7036742568016052\n",
      "batch: 563, loss: 0.849211573600769\n",
      "batch: 564, loss: 0.8495216965675354\n",
      "batch: 565, loss: 0.8496727347373962\n",
      "batch: 566, loss: 0.8496366739273071\n",
      "batch: 567, loss: 0.849603533744812\n",
      "batch: 568, loss: 0.8495965003967285\n",
      "batch: 569, loss: 0.8494375944137573\n",
      "batch: 570, loss: 0.8491233587265015\n",
      "batch: 571, loss: 0.8488131761550903\n",
      "batch: 572, loss: 0.8484400510787964\n",
      "batch: 573, loss: 0.8480831384658813\n",
      "batch: 574, loss: 0.8476790189743042\n",
      "batch: 575, loss: 0.7394018173217773\n",
      "batch: 576, loss: 0.5599781274795532\n",
      "batch: 577, loss: 0.7033733129501343\n",
      "batch: 578, loss: 0.8462638258934021\n",
      "batch: 579, loss: 0.8458926677703857\n",
      "batch: 580, loss: 0.8455712199211121\n",
      "batch: 581, loss: 0.8451535701751709\n",
      "batch: 582, loss: 0.84476637840271\n",
      "batch: 583, loss: 0.5971646308898926\n",
      "batch: 584, loss: 0.5621311068534851\n",
      "batch: 585, loss: 0.8436604738235474\n",
      "batch: 586, loss: 0.8082490563392639\n",
      "batch: 587, loss: 0.5629101991653442\n",
      "batch: 588, loss: 0.6330126523971558\n",
      "batch: 589, loss: 0.8423625230789185\n",
      "batch: 590, loss: 0.8422601819038391\n",
      "batch: 591, loss: 0.8419859409332275\n",
      "batch: 592, loss: 0.7028026580810547\n",
      "batch: 593, loss: 0.5641780495643616\n",
      "batch: 594, loss: 0.7373110055923462\n",
      "batch: 595, loss: 0.840829074382782\n",
      "batch: 596, loss: 0.6336797475814819\n",
      "batch: 597, loss: 0.5649415254592896\n",
      "batch: 598, loss: 0.8056387901306152\n",
      "batch: 599, loss: 0.839867115020752\n",
      "batch: 600, loss: 0.5652931928634644\n",
      "batch: 601, loss: 0.56541907787323\n",
      "batch: 602, loss: 0.565491259098053\n",
      "batch: 603, loss: 0.565517783164978\n",
      "batch: 604, loss: 0.5654995441436768\n",
      "batch: 605, loss: 0.5654427409172058\n",
      "batch: 606, loss: 0.5653519630432129\n",
      "batch: 607, loss: 0.7025460004806519\n",
      "batch: 608, loss: 0.8399937748908997\n",
      "batch: 609, loss: 0.6681984663009644\n",
      "batch: 610, loss: 0.5650462508201599\n",
      "batch: 611, loss: 0.564969003200531\n",
      "batch: 612, loss: 0.5648593902587891\n",
      "batch: 613, loss: 0.5647376775741577\n",
      "batch: 614, loss: 0.5645647048950195\n",
      "batch: 615, loss: 0.564367413520813\n",
      "batch: 616, loss: 0.5987971425056458\n",
      "batch: 617, loss: 0.841560423374176\n",
      "batch: 618, loss: 0.7722616195678711\n",
      "batch: 619, loss: 0.5636999011039734\n",
      "batch: 620, loss: 0.5635707974433899\n",
      "batch: 621, loss: 0.5634148716926575\n",
      "batch: 622, loss: 0.5632349848747253\n",
      "batch: 623, loss: 0.5630326271057129\n",
      "batch: 624, loss: 0.5628106594085693\n",
      "batch: 625, loss: 0.5625710487365723\n",
      "batch: 626, loss: 0.5623158812522888\n",
      "batch: 627, loss: 0.5620471239089966\n",
      "batch: 628, loss: 0.8091063499450684\n",
      "batch: 629, loss: 0.844723641872406\n",
      "batch: 630, loss: 0.5614121556282043\n",
      "batch: 631, loss: 0.596732497215271\n",
      "batch: 632, loss: 0.8453594446182251\n",
      "batch: 633, loss: 0.8454960584640503\n",
      "batch: 634, loss: 0.8455577492713928\n",
      "batch: 635, loss: 0.7388129830360413\n",
      "batch: 636, loss: 0.5609688758850098\n",
      "batch: 637, loss: 0.7032345533370972\n",
      "batch: 638, loss: 0.8455155491828918\n",
      "batch: 639, loss: 0.8454517126083374\n",
      "batch: 640, loss: 0.8453204035758972\n",
      "batch: 641, loss: 0.8451360464096069\n",
      "batch: 642, loss: 0.8448982238769531\n",
      "batch: 643, loss: 0.8446145057678223\n",
      "batch: 644, loss: 0.8442877531051636\n",
      "batch: 645, loss: 0.5621518492698669\n",
      "batch: 646, loss: 0.5975198149681091\n",
      "batch: 647, loss: 0.8434392809867859\n",
      "batch: 648, loss: 0.7730597257614136\n",
      "batch: 649, loss: 0.5629155039787292\n",
      "batch: 650, loss: 0.6679339408874512\n",
      "batch: 651, loss: 0.8425583839416504\n",
      "batch: 652, loss: 0.8423393964767456\n",
      "batch: 653, loss: 0.842072606086731\n",
      "batch: 654, loss: 0.8417630195617676\n",
      "batch: 655, loss: 0.8414158225059509\n",
      "batch: 656, loss: 0.633513867855072\n",
      "batch: 657, loss: 0.5645849108695984\n",
      "batch: 658, loss: 0.8060050010681152\n",
      "batch: 659, loss: 0.8401885032653809\n",
      "batch: 660, loss: 0.8398903608322144\n",
      "batch: 661, loss: 0.8052734136581421\n",
      "batch: 662, loss: 0.5657483339309692\n",
      "batch: 663, loss: 0.6341953873634338\n",
      "batch: 664, loss: 0.8386726975440979\n",
      "batch: 665, loss: 0.8383976221084595\n",
      "batch: 666, loss: 0.8380787968635559\n",
      "batch: 667, loss: 0.8377261161804199\n",
      "batch: 668, loss: 0.8373339772224426\n",
      "batch: 669, loss: 0.8369154930114746\n",
      "batch: 670, loss: 0.836468517780304\n",
      "batch: 671, loss: 0.8359976410865784\n",
      "batch: 672, loss: 0.835506796836853\n",
      "batch: 673, loss: 0.6022028923034668\n",
      "batch: 674, loss: 0.5692815184593201\n",
      "batch: 675, loss: 0.5695115923881531\n",
      "batch: 676, loss: 0.602738082408905\n",
      "batch: 677, loss: 0.8338208198547363\n",
      "batch: 678, loss: 0.7677145004272461\n",
      "batch: 679, loss: 0.5702579617500305\n",
      "batch: 680, loss: 0.5703648328781128\n",
      "batch: 681, loss: 0.5704067349433899\n",
      "batch: 682, loss: 0.570446252822876\n",
      "batch: 683, loss: 0.5704158544540405\n",
      "batch: 684, loss: 0.5703698396682739\n",
      "batch: 685, loss: 0.570273756980896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 686, loss: 0.5701514482498169\n",
      "batch: 687, loss: 0.5699998140335083\n",
      "batch: 688, loss: 0.5698595643043518\n",
      "batch: 689, loss: 0.5696487426757812\n",
      "batch: 690, loss: 0.5694072246551514\n",
      "batch: 691, loss: 0.5691702961921692\n",
      "batch: 692, loss: 0.5689184069633484\n",
      "batch: 693, loss: 0.5686418414115906\n",
      "batch: 694, loss: 0.5683610439300537\n",
      "batch: 695, loss: 0.5680619478225708\n",
      "batch: 696, loss: 0.5677560567855835\n",
      "batch: 697, loss: 0.5674417614936829\n",
      "batch: 698, loss: 0.5671175718307495\n",
      "batch: 699, loss: 0.56678307056427\n",
      "batch: 700, loss: 0.566443920135498\n",
      "batch: 701, loss: 0.5660985112190247\n",
      "batch: 702, loss: 0.5657480359077454\n",
      "batch: 703, loss: 0.8053515553474426\n",
      "batch: 704, loss: 0.8400129675865173\n",
      "batch: 705, loss: 0.8402709364891052\n",
      "batch: 706, loss: 0.8059737682342529\n",
      "batch: 707, loss: 0.5647161602973938\n",
      "batch: 708, loss: 0.5646128058433533\n",
      "batch: 709, loss: 0.5644816160202026\n",
      "batch: 710, loss: 0.5643175840377808\n",
      "batch: 711, loss: 0.5641318559646606\n",
      "batch: 712, loss: 0.7027536630630493\n",
      "batch: 713, loss: 0.84181809425354\n",
      "batch: 714, loss: 0.8419594764709473\n",
      "batch: 715, loss: 0.8420209884643555\n",
      "batch: 716, loss: 0.842004120349884\n",
      "batch: 717, loss: 0.8419197797775269\n",
      "batch: 718, loss: 0.5985283851623535\n",
      "batch: 719, loss: 0.5638508796691895\n",
      "batch: 720, loss: 0.8416471481323242\n",
      "batch: 721, loss: 0.8415465354919434\n",
      "batch: 722, loss: 0.8413898944854736\n",
      "batch: 723, loss: 0.8411785364151001\n",
      "batch: 724, loss: 0.8409169316291809\n",
      "batch: 725, loss: 0.8406128287315369\n",
      "batch: 726, loss: 0.8402614593505859\n",
      "batch: 727, loss: 0.8398922085762024\n",
      "batch: 728, loss: 0.839483380317688\n",
      "batch: 729, loss: 0.8390462398529053\n",
      "batch: 730, loss: 0.8385813236236572\n",
      "batch: 731, loss: 0.8380990028381348\n",
      "batch: 732, loss: 0.8375916481018066\n",
      "batch: 733, loss: 0.6010705232620239\n",
      "batch: 734, loss: 0.5676881670951843\n",
      "batch: 735, loss: 0.8362950086593628\n",
      "batch: 736, loss: 0.8359229564666748\n",
      "batch: 737, loss: 0.8355197310447693\n",
      "batch: 738, loss: 0.8350867033004761\n",
      "batch: 739, loss: 0.834631085395813\n",
      "batch: 740, loss: 0.8341498970985413\n",
      "batch: 741, loss: 0.8336460590362549\n",
      "batch: 742, loss: 0.8331286907196045\n",
      "batch: 743, loss: 0.832588791847229\n",
      "batch: 744, loss: 0.8320425152778625\n",
      "batch: 745, loss: 0.8314835429191589\n",
      "batch: 746, loss: 0.8309089541435242\n",
      "batch: 747, loss: 0.8303264379501343\n",
      "batch: 748, loss: 0.6050880551338196\n",
      "batch: 749, loss: 0.5733857154846191\n",
      "batch: 750, loss: 0.5736852884292603\n",
      "batch: 751, loss: 0.5739186406135559\n",
      "batch: 752, loss: 0.5740889310836792\n",
      "batch: 753, loss: 0.5742010474205017\n",
      "batch: 754, loss: 0.5742611289024353\n",
      "batch: 755, loss: 0.5742759108543396\n",
      "batch: 756, loss: 0.5742453336715698\n",
      "batch: 757, loss: 0.5741800665855408\n",
      "batch: 758, loss: 0.5740783214569092\n",
      "batch: 759, loss: 0.5739454627037048\n",
      "batch: 760, loss: 0.5737863779067993\n",
      "batch: 761, loss: 0.5736024379730225\n",
      "batch: 762, loss: 0.5733970403671265\n",
      "batch: 763, loss: 0.5731696486473083\n",
      "batch: 764, loss: 0.5729236602783203\n",
      "batch: 765, loss: 0.5726631879806519\n",
      "batch: 766, loss: 0.5723873972892761\n",
      "batch: 767, loss: 0.5720981359481812\n",
      "batch: 768, loss: 0.571797788143158\n",
      "batch: 769, loss: 0.5714870095252991\n",
      "batch: 770, loss: 0.5711671113967896\n",
      "batch: 771, loss: 0.5708401203155518\n",
      "batch: 772, loss: 0.5705025792121887\n",
      "batch: 773, loss: 0.5701585412025452\n",
      "batch: 774, loss: 0.5698111057281494\n",
      "batch: 775, loss: 0.5694578289985657\n",
      "batch: 776, loss: 0.5690972208976746\n",
      "batch: 777, loss: 0.5687321424484253\n",
      "batch: 778, loss: 0.8023263812065125\n",
      "batch: 779, loss: 0.8361279368400574\n",
      "batch: 780, loss: 0.5678678154945374\n",
      "batch: 781, loss: 0.5676402449607849\n",
      "batch: 782, loss: 0.5673955678939819\n",
      "batch: 783, loss: 0.5671352744102478\n",
      "batch: 784, loss: 0.5668608546257019\n",
      "batch: 785, loss: 0.5665737986564636\n",
      "batch: 786, loss: 0.5662755966186523\n",
      "batch: 787, loss: 0.7024294137954712\n",
      "batch: 788, loss: 0.8392477631568909\n",
      "batch: 789, loss: 0.66825270652771\n",
      "batch: 790, loss: 0.5653262734413147\n",
      "batch: 791, loss: 0.7712793946266174\n",
      "batch: 792, loss: 0.8401942253112793\n",
      "batch: 793, loss: 0.8403042554855347\n",
      "batch: 794, loss: 0.8403328657150269\n",
      "batch: 795, loss: 0.5649057626724243\n",
      "batch: 796, loss: 0.5648949146270752\n",
      "batch: 797, loss: 0.5648478269577026\n",
      "batch: 798, loss: 0.6336910128593445\n",
      "batch: 799, loss: 0.8406001329421997\n",
      "batch: 800, loss: 0.8406406044960022\n",
      "batch: 801, loss: 0.8406075835227966\n",
      "batch: 802, loss: 0.8405080437660217\n",
      "batch: 803, loss: 0.8403500914573669\n",
      "batch: 804, loss: 0.8401383757591248\n",
      "batch: 805, loss: 0.8398785591125488\n",
      "batch: 806, loss: 0.8395757675170898\n",
      "batch: 807, loss: 0.8392341732978821\n",
      "batch: 808, loss: 0.6001009941101074\n",
      "batch: 809, loss: 0.5662224292755127\n",
      "batch: 810, loss: 0.5663890242576599\n",
      "batch: 811, loss: 0.6004606485366821\n",
      "batch: 812, loss: 0.8380990624427795\n",
      "batch: 813, loss: 0.8379480242729187\n",
      "batch: 814, loss: 0.8377432823181152\n",
      "batch: 815, loss: 0.8374891877174377\n",
      "batch: 816, loss: 0.8371917009353638\n",
      "batch: 817, loss: 0.8368553519248962\n",
      "batch: 818, loss: 0.8364834785461426\n",
      "batch: 819, loss: 0.8360810875892639\n",
      "batch: 820, loss: 0.8356500267982483\n",
      "batch: 821, loss: 0.8351936936378479\n",
      "batch: 822, loss: 0.8347148299217224\n",
      "batch: 823, loss: 0.8342151641845703\n",
      "batch: 824, loss: 0.8336978554725647\n",
      "batch: 825, loss: 0.5703637599945068\n",
      "batch: 826, loss: 0.5706971883773804\n",
      "batch: 827, loss: 0.5709494948387146\n",
      "batch: 828, loss: 0.5711320638656616\n",
      "batch: 829, loss: 0.5712608098983765\n",
      "batch: 830, loss: 0.5713370442390442\n",
      "batch: 831, loss: 0.5713648200035095\n",
      "batch: 832, loss: 0.7016061544418335\n",
      "batch: 833, loss: 0.8318716883659363\n",
      "batch: 834, loss: 0.6690459847450256\n",
      "batch: 835, loss: 0.5714223384857178\n",
      "batch: 836, loss: 0.571415901184082\n",
      "batch: 837, loss: 0.571366012096405\n",
      "batch: 838, loss: 0.5712855458259583\n",
      "batch: 839, loss: 0.5711710453033447\n",
      "batch: 840, loss: 0.5710223913192749\n",
      "batch: 841, loss: 0.6035597920417786\n",
      "batch: 842, loss: 0.832750141620636\n",
      "batch: 843, loss: 0.7673084139823914\n",
      "batch: 844, loss: 0.5704849362373352\n",
      "batch: 845, loss: 0.5703802108764648\n",
      "batch: 846, loss: 0.5702449679374695\n",
      "batch: 847, loss: 0.570083498954773\n",
      "batch: 848, loss: 0.5698977112770081\n",
      "batch: 849, loss: 0.5696900486946106\n",
      "batch: 850, loss: 0.5694631934165955\n",
      "batch: 851, loss: 0.5692183375358582\n",
      "batch: 852, loss: 0.5689573287963867\n",
      "batch: 853, loss: 0.5686827301979065\n",
      "batch: 854, loss: 0.5683949589729309\n",
      "batch: 855, loss: 0.8360996842384338\n",
      "batch: 856, loss: 0.8028215169906616\n",
      "batch: 857, loss: 0.5677260160446167\n",
      "batch: 858, loss: 0.5675479173660278\n",
      "batch: 859, loss: 0.5673465728759766\n",
      "batch: 860, loss: 0.5671256184577942\n",
      "batch: 861, loss: 0.5668867826461792\n",
      "batch: 862, loss: 0.7023242115974426\n",
      "batch: 863, loss: 0.8383119106292725\n",
      "batch: 864, loss: 0.8385067582130432\n",
      "batch: 865, loss: 0.8386123180389404\n",
      "batch: 866, loss: 0.8386391401290894\n",
      "batch: 867, loss: 0.8385919332504272\n",
      "batch: 868, loss: 0.8384843468666077\n",
      "batch: 869, loss: 0.8383163809776306\n",
      "batch: 870, loss: 0.838093638420105\n",
      "batch: 871, loss: 0.803948163986206\n",
      "batch: 872, loss: 0.5669999122619629\n",
      "batch: 873, loss: 0.5671622157096863\n",
      "batch: 874, loss: 0.5672669410705566\n",
      "batch: 875, loss: 0.5673216581344604\n",
      "batch: 876, loss: 0.567331850528717\n",
      "batch: 877, loss: 0.7022216320037842\n",
      "batch: 878, loss: 0.8371739387512207\n",
      "batch: 879, loss: 0.8371299505233765\n",
      "batch: 880, loss: 0.8370249271392822\n",
      "batch: 881, loss: 0.8368591070175171\n",
      "batch: 882, loss: 0.8366408944129944\n",
      "batch: 883, loss: 0.8363784551620483\n",
      "batch: 884, loss: 0.8360722064971924\n",
      "batch: 885, loss: 0.835727870464325\n",
      "batch: 886, loss: 0.8353497385978699\n",
      "batch: 887, loss: 0.8349411487579346\n",
      "batch: 888, loss: 0.8345047235488892\n",
      "batch: 889, loss: 0.8340440392494202\n",
      "batch: 890, loss: 0.8335608243942261\n",
      "batch: 891, loss: 0.8330588340759277\n",
      "batch: 892, loss: 0.8325386047363281\n",
      "batch: 893, loss: 0.8320023417472839\n",
      "batch: 894, loss: 0.8314525485038757\n",
      "batch: 895, loss: 0.8308859467506409\n",
      "batch: 896, loss: 0.8303144574165344\n",
      "batch: 897, loss: 0.8297314643859863\n",
      "batch: 898, loss: 0.6054149270057678\n",
      "batch: 899, loss: 0.5738397240638733\n",
      "batch: 900, loss: 0.5741453766822815\n",
      "batch: 901, loss: 0.6060755252838135\n",
      "batch: 902, loss: 0.8277110457420349\n",
      "batch: 903, loss: 0.7642688751220703\n",
      "batch: 904, loss: 0.5750021934509277\n",
      "batch: 905, loss: 0.575162947177887\n",
      "batch: 906, loss: 0.5752668976783752\n",
      "batch: 907, loss: 0.5753197073936462\n",
      "batch: 908, loss: 0.5753263831138611\n",
      "batch: 909, loss: 0.7324666976928711\n",
      "batch: 910, loss: 0.826789915561676\n",
      "batch: 911, loss: 0.8267384171485901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 912, loss: 0.8266247510910034\n",
      "batch: 913, loss: 0.6069028973579407\n",
      "batch: 914, loss: 0.5756282210350037\n",
      "batch: 915, loss: 0.5756682753562927\n",
      "batch: 916, loss: 0.6069921255111694\n",
      "batch: 917, loss: 0.8263373374938965\n",
      "batch: 918, loss: 0.7636444568634033\n",
      "batch: 919, loss: 0.5757011771202087\n",
      "batch: 920, loss: 0.5757045149803162\n",
      "batch: 921, loss: 0.5756667256355286\n",
      "batch: 922, loss: 0.5755918025970459\n",
      "batch: 923, loss: 0.5754833817481995\n",
      "batch: 924, loss: 0.575344979763031\n",
      "batch: 925, loss: 0.5751795172691345\n",
      "batch: 926, loss: 0.7641175985336304\n",
      "batch: 927, loss: 0.82734215259552\n",
      "batch: 928, loss: 0.827438473701477\n",
      "batch: 929, loss: 0.8274574875831604\n",
      "batch: 930, loss: 0.574798047542572\n",
      "batch: 931, loss: 0.5747924447059631\n",
      "batch: 932, loss: 0.5747466683387756\n",
      "batch: 933, loss: 0.5746647715568542\n",
      "batch: 934, loss: 0.574550211429596\n",
      "batch: 935, loss: 0.5744062066078186\n",
      "batch: 936, loss: 0.5742359757423401\n",
      "batch: 937, loss: 0.7012115120887756\n",
      "batch: 938, loss: 0.8285988569259644\n",
      "batch: 939, loss: 0.8287267684936523\n",
      "batch: 940, loss: 0.828774094581604\n",
      "batch: 941, loss: 0.8287490606307983\n",
      "batch: 942, loss: 0.8286585807800293\n",
      "batch: 943, loss: 0.8285093903541565\n",
      "batch: 944, loss: 0.828307569026947\n",
      "batch: 945, loss: 0.8280582427978516\n",
      "batch: 946, loss: 0.7961103916168213\n",
      "batch: 947, loss: 0.5747637748718262\n",
      "batch: 948, loss: 0.574943482875824\n",
      "batch: 949, loss: 0.5750642418861389\n",
      "batch: 950, loss: 0.5751323103904724\n",
      "batch: 951, loss: 0.5751526951789856\n",
      "batch: 952, loss: 0.7010548114776611\n",
      "batch: 953, loss: 0.8269978761672974\n",
      "batch: 954, loss: 0.8269470930099487\n",
      "batch: 955, loss: 0.8268336653709412\n",
      "batch: 956, loss: 0.8266640305519104\n",
      "batch: 957, loss: 0.8264440894126892\n",
      "batch: 958, loss: 0.8261786103248596\n",
      "batch: 959, loss: 0.8258724212646484\n",
      "batch: 960, loss: 0.8255295753479004\n",
      "batch: 961, loss: 0.8251538872718811\n",
      "batch: 962, loss: 0.8247485756874084\n",
      "batch: 963, loss: 0.8243169188499451\n",
      "batch: 964, loss: 0.8238615393638611\n",
      "batch: 965, loss: 0.8233846426010132\n",
      "batch: 966, loss: 0.8228887915611267\n",
      "batch: 967, loss: 0.8223757743835449\n",
      "batch: 968, loss: 0.8218474388122559\n",
      "batch: 969, loss: 0.8213056325912476\n",
      "batch: 970, loss: 0.8207513093948364\n",
      "batch: 971, loss: 0.820186197757721\n",
      "batch: 972, loss: 0.8196114301681519\n",
      "batch: 973, loss: 0.8190277814865112\n",
      "batch: 974, loss: 0.8184365034103394\n",
      "batch: 975, loss: 0.5822944641113281\n",
      "batch: 976, loss: 0.5826783776283264\n",
      "batch: 977, loss: 0.5829827785491943\n",
      "batch: 978, loss: 0.5832152366638184\n",
      "batch: 979, loss: 0.5833830237388611\n",
      "batch: 980, loss: 0.5834925174713135\n",
      "batch: 981, loss: 0.5835494995117188\n",
      "batch: 982, loss: 0.699899435043335\n",
      "batch: 983, loss: 0.8162217140197754\n",
      "batch: 984, loss: 0.6708266139030457\n",
      "batch: 985, loss: 0.5836911797523499\n",
      "batch: 986, loss: 0.5836969017982483\n",
      "batch: 987, loss: 0.583660364151001\n",
      "batch: 988, loss: 0.583585798740387\n",
      "batch: 989, loss: 0.5834770798683167\n",
      "batch: 990, loss: 0.5833377838134766\n",
      "batch: 991, loss: 0.6123658418655396\n",
      "batch: 992, loss: 0.8169575929641724\n",
      "batch: 993, loss: 0.7585424780845642\n",
      "batch: 994, loss: 0.5828115344047546\n",
      "batch: 995, loss: 0.5827073454856873\n",
      "batch: 996, loss: 0.5825720429420471\n",
      "batch: 997, loss: 0.5824088454246521\n",
      "batch: 998, loss: 0.5822203755378723\n",
      "batch: 999, loss: 0.5820093154907227\n",
      "batch: 1000, loss: 0.5817779898643494\n",
      "batch: 1001, loss: 0.58152836561203\n",
      "batch: 1002, loss: 0.5812624096870422\n",
      "batch: 1003, loss: 0.7896870374679565\n",
      "batch: 1004, loss: 0.8197709321975708\n",
      "batch: 1005, loss: 0.5806317329406738\n",
      "batch: 1006, loss: 0.5804662108421326\n",
      "batch: 1007, loss: 0.5802759528160095\n",
      "batch: 1008, loss: 0.5800634026527405\n",
      "batch: 1009, loss: 0.5798308253288269\n",
      "batch: 1010, loss: 0.5795803666114807\n",
      "batch: 1011, loss: 0.5793138146400452\n",
      "batch: 1012, loss: 0.7005071043968201\n",
      "batch: 1013, loss: 0.8222969770431519\n",
      "batch: 1014, loss: 0.8225136995315552\n",
      "batch: 1015, loss: 0.8226418495178223\n",
      "batch: 1016, loss: 0.8226901292800903\n",
      "batch: 1017, loss: 0.8226667046546936\n",
      "batch: 1018, loss: 0.8225785493850708\n",
      "batch: 1019, loss: 0.8224323987960815\n",
      "batch: 1020, loss: 0.8222337961196899\n",
      "batch: 1021, loss: 0.8219884634017944\n",
      "batch: 1022, loss: 0.821700930595398\n",
      "batch: 1023, loss: 0.7609085440635681\n",
      "batch: 1024, loss: 0.5797677040100098\n",
      "batch: 1025, loss: 0.6702752113342285\n",
      "batch: 1026, loss: 0.820588231086731\n",
      "batch: 1027, loss: 0.8203306198120117\n",
      "batch: 1028, loss: 0.8200321197509766\n",
      "batch: 1029, loss: 0.819696843624115\n",
      "batch: 1030, loss: 0.819328784942627\n",
      "batch: 1031, loss: 0.8189310431480408\n",
      "batch: 1032, loss: 0.8185067772865295\n",
      "batch: 1033, loss: 0.8180589079856873\n",
      "batch: 1034, loss: 0.8175896406173706\n",
      "batch: 1035, loss: 0.8171011209487915\n",
      "batch: 1036, loss: 0.8165956139564514\n",
      "batch: 1037, loss: 0.816074788570404\n",
      "batch: 1038, loss: 0.8155401945114136\n",
      "batch: 1039, loss: 0.8149933218955994\n",
      "batch: 1040, loss: 0.8144354820251465\n",
      "batch: 1041, loss: 0.8138678669929504\n",
      "batch: 1042, loss: 0.813291609287262\n",
      "batch: 1043, loss: 0.8127075433731079\n",
      "batch: 1044, loss: 0.8121166229248047\n",
      "batch: 1045, loss: 0.8115196228027344\n",
      "batch: 1046, loss: 0.6435770988464355\n",
      "batch: 1047, loss: 0.588212788105011\n",
      "batch: 1048, loss: 0.5885452628135681\n",
      "batch: 1049, loss: 0.5888023376464844\n",
      "batch: 1050, loss: 0.58899986743927\n",
      "batch: 1051, loss: 0.5891240835189819\n",
      "batch: 1052, loss: 0.5891959071159363\n",
      "batch: 1053, loss: 0.5892240405082703\n",
      "batch: 1054, loss: 0.5892002582550049\n",
      "batch: 1055, loss: 0.589147686958313\n",
      "batch: 1056, loss: 0.5890417695045471\n",
      "batch: 1057, loss: 0.588912844657898\n",
      "batch: 1058, loss: 0.5887563228607178\n",
      "batch: 1059, loss: 0.5885733366012573\n",
      "batch: 1060, loss: 0.5883629322052002\n",
      "batch: 1061, loss: 0.5881341099739075\n",
      "batch: 1062, loss: 0.5878860950469971\n",
      "batch: 1063, loss: 0.5876230597496033\n",
      "batch: 1064, loss: 0.5873406529426575\n",
      "batch: 1065, loss: 0.5870478749275208\n",
      "batch: 1066, loss: 0.6149275898933411\n",
      "batch: 1067, loss: 0.8126219511032104\n",
      "batch: 1068, loss: 0.7562301158905029\n",
      "batch: 1069, loss: 0.5860396027565002\n",
      "batch: 1070, loss: 0.5858433842658997\n",
      "batch: 1071, loss: 0.5856251120567322\n",
      "batch: 1072, loss: 0.5853868126869202\n",
      "batch: 1073, loss: 0.5851351618766785\n",
      "batch: 1074, loss: 0.5848602056503296\n",
      "batch: 1075, loss: 0.5845723748207092\n",
      "batch: 1076, loss: 0.5842728614807129\n",
      "batch: 1077, loss: 0.5839619636535645\n",
      "batch: 1078, loss: 0.7870750427246094\n",
      "batch: 1079, loss: 0.8164505958557129\n",
      "batch: 1080, loss: 0.5832211375236511\n",
      "batch: 1081, loss: 0.5830264091491699\n",
      "batch: 1082, loss: 0.582808256149292\n",
      "batch: 1083, loss: 0.582571268081665\n",
      "batch: 1084, loss: 0.5823165774345398\n",
      "batch: 1085, loss: 0.5820481181144714\n",
      "batch: 1086, loss: 0.5817610025405884\n",
      "batch: 1087, loss: 0.7001758813858032\n",
      "batch: 1088, loss: 0.8192232251167297\n",
      "batch: 1089, loss: 0.8194565176963806\n",
      "batch: 1090, loss: 0.8196001052856445\n",
      "batch: 1091, loss: 0.8196624517440796\n",
      "batch: 1092, loss: 0.8196520805358887\n",
      "batch: 1093, loss: 0.819575846195221\n",
      "batch: 1094, loss: 0.8194409608840942\n",
      "batch: 1095, loss: 0.8192533254623413\n",
      "batch: 1096, loss: 0.8190159797668457\n",
      "batch: 1097, loss: 0.8187398314476013\n",
      "batch: 1098, loss: 0.7592750787734985\n",
      "batch: 1099, loss: 0.5820861458778381\n",
      "batch: 1100, loss: 0.6706202030181885\n",
      "batch: 1101, loss: 0.8176588416099548\n",
      "batch: 1102, loss: 0.8174035549163818\n",
      "batch: 1103, loss: 0.8171154260635376\n",
      "batch: 1104, loss: 0.8167746663093567\n",
      "batch: 1105, loss: 0.8164243698120117\n",
      "batch: 1106, loss: 0.816031813621521\n",
      "batch: 1107, loss: 0.8156149387359619\n",
      "batch: 1108, loss: 0.8151723742485046\n",
      "batch: 1109, loss: 0.8147087097167969\n",
      "batch: 1110, loss: 0.8142244815826416\n",
      "batch: 1111, loss: 0.8137248754501343\n",
      "batch: 1112, loss: 0.8132096529006958\n",
      "batch: 1113, loss: 0.7561073899269104\n",
      "batch: 1114, loss: 0.5867977738380432\n",
      "batch: 1115, loss: 0.6713614463806152\n",
      "batch: 1116, loss: 0.8113989233970642\n",
      "batch: 1117, loss: 0.8110095262527466\n",
      "batch: 1118, loss: 0.8105912208557129\n",
      "batch: 1119, loss: 0.8101547956466675\n",
      "batch: 1120, loss: 0.8096943497657776\n",
      "batch: 1121, loss: 0.8092150688171387\n",
      "batch: 1122, loss: 0.8087151050567627\n",
      "batch: 1123, loss: 0.8081923723220825\n",
      "batch: 1124, loss: 0.8076775670051575\n",
      "batch: 1125, loss: 0.807144045829773\n",
      "batch: 1126, loss: 0.7796796560287476\n",
      "batch: 1127, loss: 0.5917065143585205\n",
      "batch: 1128, loss: 0.5920591354370117\n",
      "batch: 1129, loss: 0.5923343300819397\n",
      "batch: 1130, loss: 0.5925397872924805\n",
      "batch: 1131, loss: 0.592682421207428\n",
      "batch: 1132, loss: 0.5927684307098389\n",
      "batch: 1133, loss: 0.5928035378456116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1134, loss: 0.725239634513855\n",
      "batch: 1135, loss: 0.8046988844871521\n",
      "batch: 1136, loss: 0.6458005905151367\n",
      "batch: 1137, loss: 0.5928928852081299\n",
      "batch: 1138, loss: 0.7781336903572083\n",
      "batch: 1139, loss: 0.8045600652694702\n",
      "batch: 1140, loss: 0.8044627904891968\n",
      "batch: 1141, loss: 0.7779102921485901\n",
      "batch: 1142, loss: 0.593267023563385\n",
      "batch: 1143, loss: 0.5933616161346436\n",
      "batch: 1144, loss: 0.5934051275253296\n",
      "batch: 1145, loss: 0.5934000611305237\n",
      "batch: 1146, loss: 0.5933539271354675\n",
      "batch: 1147, loss: 0.6986939907073975\n",
      "batch: 1148, loss: 0.8042054176330566\n",
      "batch: 1149, loss: 0.8042191863059998\n",
      "batch: 1150, loss: 0.8041667342185974\n",
      "batch: 1151, loss: 0.804054856300354\n",
      "batch: 1152, loss: 0.8038896918296814\n",
      "batch: 1153, loss: 0.803676187992096\n",
      "batch: 1154, loss: 0.8034195899963379\n",
      "batch: 1155, loss: 0.8031240105628967\n",
      "batch: 1156, loss: 0.776737630367279\n",
      "batch: 1157, loss: 0.5946294665336609\n",
      "batch: 1158, loss: 0.646674394607544\n",
      "batch: 1159, loss: 0.8019757866859436\n",
      "batch: 1160, loss: 0.8017216324806213\n",
      "batch: 1161, loss: 0.8014287352561951\n",
      "batch: 1162, loss: 0.8011007308959961\n",
      "batch: 1163, loss: 0.8007411956787109\n",
      "batch: 1164, loss: 0.6728384494781494\n",
      "batch: 1165, loss: 0.5966066718101501\n",
      "batch: 1166, loss: 0.7490233778953552\n",
      "batch: 1167, loss: 0.7994964122772217\n",
      "batch: 1168, loss: 0.7991951704025269\n",
      "batch: 1169, loss: 0.7988588213920593\n",
      "batch: 1170, loss: 0.7984946966171265\n",
      "batch: 1171, loss: 0.7981017827987671\n",
      "batch: 1172, loss: 0.797683596611023\n",
      "batch: 1173, loss: 0.7972435355186462\n",
      "batch: 1174, loss: 0.7967858910560608\n",
      "batch: 1175, loss: 0.796309232711792\n",
      "batch: 1176, loss: 0.7958164811134338\n",
      "batch: 1177, loss: 0.7953099012374878\n",
      "batch: 1178, loss: 0.7947889566421509\n",
      "batch: 1179, loss: 0.6736762523651123\n",
      "batch: 1180, loss: 0.6017128825187683\n",
      "batch: 1181, loss: 0.7455703020095825\n",
      "batch: 1182, loss: 0.7930543422698975\n",
      "batch: 1183, loss: 0.6263981461524963\n",
      "batch: 1184, loss: 0.6029078960418701\n",
      "batch: 1185, loss: 0.7921125888824463\n",
      "batch: 1186, loss: 0.7918404340744019\n",
      "batch: 1187, loss: 0.7915322780609131\n",
      "batch: 1188, loss: 0.7911913394927979\n",
      "batch: 1189, loss: 0.7908223867416382\n",
      "batch: 1190, loss: 0.7207033038139343\n",
      "batch: 1191, loss: 0.6048110723495483\n",
      "batch: 1192, loss: 0.6050483584403992\n",
      "batch: 1193, loss: 0.6052173972129822\n",
      "batch: 1194, loss: 0.7203902006149292\n",
      "batch: 1195, loss: 0.7892905473709106\n",
      "batch: 1196, loss: 0.6514742374420166\n",
      "batch: 1197, loss: 0.6057188510894775\n",
      "batch: 1198, loss: 0.7659927010536194\n",
      "batch: 1199, loss: 0.7887606620788574\n",
      "batch: 1200, loss: 0.7885897159576416\n",
      "batch: 1201, loss: 0.7656017541885376\n",
      "batch: 1202, loss: 0.6064087748527527\n",
      "batch: 1203, loss: 0.6065463423728943\n",
      "batch: 1204, loss: 0.6066274046897888\n",
      "batch: 1205, loss: 0.6066592931747437\n",
      "batch: 1206, loss: 0.6066380143165588\n",
      "batch: 1207, loss: 0.6065803170204163\n",
      "batch: 1208, loss: 0.6064823865890503\n",
      "batch: 1209, loss: 0.6063506007194519\n",
      "batch: 1210, loss: 0.6061891317367554\n",
      "batch: 1211, loss: 0.6059974431991577\n",
      "batch: 1212, loss: 0.6057829856872559\n",
      "batch: 1213, loss: 0.6055487394332886\n",
      "batch: 1214, loss: 0.6052901744842529\n",
      "batch: 1215, loss: 0.6050153970718384\n",
      "batch: 1216, loss: 0.6047245860099792\n",
      "batch: 1217, loss: 0.6044192910194397\n",
      "batch: 1218, loss: 0.6041018962860107\n",
      "batch: 1219, loss: 0.603773832321167\n",
      "batch: 1220, loss: 0.6034359931945801\n",
      "batch: 1221, loss: 0.6030838489532471\n",
      "batch: 1222, loss: 0.6976467967033386\n",
      "batch: 1223, loss: 0.7929502725601196\n",
      "batch: 1224, loss: 0.6738219261169434\n",
      "batch: 1225, loss: 0.6019559502601624\n",
      "batch: 1226, loss: 0.6017169952392578\n",
      "batch: 1227, loss: 0.6014591455459595\n",
      "batch: 1228, loss: 0.6011857390403748\n",
      "batch: 1229, loss: 0.6008937358856201\n",
      "batch: 1230, loss: 0.7951565384864807\n",
      "batch: 1231, loss: 0.7710444927215576\n",
      "batch: 1232, loss: 0.6002044677734375\n",
      "batch: 1233, loss: 0.6000178456306458\n",
      "batch: 1234, loss: 0.59980708360672\n",
      "batch: 1235, loss: 0.5995736122131348\n",
      "batch: 1236, loss: 0.599320650100708\n",
      "batch: 1237, loss: 0.6980385780334473\n",
      "batch: 1238, loss: 0.7973183989524841\n",
      "batch: 1239, loss: 0.7975162863731384\n",
      "batch: 1240, loss: 0.797630250453949\n",
      "batch: 1241, loss: 0.7976693511009216\n",
      "batch: 1242, loss: 0.7976394295692444\n",
      "batch: 1243, loss: 0.7975490689277649\n",
      "batch: 1244, loss: 0.7974043488502502\n",
      "batch: 1245, loss: 0.5989002585411072\n",
      "batch: 1246, loss: 0.6237617135047913\n",
      "batch: 1247, loss: 0.7970139384269714\n",
      "batch: 1248, loss: 0.7968837022781372\n",
      "batch: 1249, loss: 0.7967047691345215\n",
      "batch: 1250, loss: 0.7226117849349976\n",
      "batch: 1251, loss: 0.5996842980384827\n",
      "batch: 1252, loss: 0.5998063087463379\n",
      "batch: 1253, loss: 0.5998733043670654\n",
      "batch: 1254, loss: 0.7224619388580322\n",
      "batch: 1255, loss: 0.7959646582603455\n",
      "batch: 1256, loss: 0.7958655953407288\n",
      "batch: 1257, loss: 0.7957127094268799\n",
      "batch: 1258, loss: 0.7955112457275391\n",
      "batch: 1259, loss: 0.7952662706375122\n",
      "batch: 1260, loss: 0.7949820756912231\n",
      "batch: 1261, loss: 0.7946627140045166\n",
      "batch: 1262, loss: 0.7943116426467896\n",
      "batch: 1263, loss: 0.7939324378967285\n",
      "batch: 1264, loss: 0.7935275435447693\n",
      "batch: 1265, loss: 0.7930998802185059\n",
      "batch: 1266, loss: 0.7926516532897949\n",
      "batch: 1267, loss: 0.7921852469444275\n",
      "batch: 1268, loss: 0.7917022109031677\n",
      "batch: 1269, loss: 0.7912036180496216\n",
      "batch: 1270, loss: 0.7906920909881592\n",
      "batch: 1271, loss: 0.6510753631591797\n",
      "batch: 1272, loss: 0.6050814986228943\n",
      "batch: 1273, loss: 0.6053722500801086\n",
      "batch: 1274, loss: 0.6055904030799866\n",
      "batch: 1275, loss: 0.6057429909706116\n",
      "batch: 1276, loss: 0.6058368682861328\n",
      "batch: 1277, loss: 0.6058775186538696\n",
      "batch: 1278, loss: 0.6058703660964966\n",
      "batch: 1279, loss: 0.6058202981948853\n",
      "batch: 1280, loss: 0.6057315468788147\n",
      "batch: 1281, loss: 0.6056079864501953\n",
      "batch: 1282, loss: 0.6054531931877136\n",
      "batch: 1283, loss: 0.6052702069282532\n",
      "batch: 1284, loss: 0.6050617694854736\n",
      "batch: 1285, loss: 0.6048307418823242\n",
      "batch: 1286, loss: 0.6045792102813721\n",
      "batch: 1287, loss: 0.6043092608451843\n",
      "batch: 1288, loss: 0.7676266431808472\n",
      "batch: 1289, loss: 0.7912606596946716\n",
      "batch: 1290, loss: 0.6036626696586609\n",
      "batch: 1291, loss: 0.6034906506538391\n",
      "batch: 1292, loss: 0.6032922863960266\n",
      "batch: 1293, loss: 0.6030704379081726\n",
      "batch: 1294, loss: 0.602827250957489\n",
      "batch: 1295, loss: 0.6025652289390564\n",
      "batch: 1296, loss: 0.6022859215736389\n",
      "batch: 1297, loss: 0.6019912362098694\n",
      "batch: 1298, loss: 0.6016830205917358\n",
      "batch: 1299, loss: 0.6013622879981995\n",
      "batch: 1300, loss: 0.6010305285453796\n",
      "batch: 1301, loss: 0.6006888151168823\n",
      "batch: 1302, loss: 0.6003381013870239\n",
      "batch: 1303, loss: 0.7714056372642517\n",
      "batch: 1304, loss: 0.7962384223937988\n",
      "batch: 1305, loss: 0.5994961261749268\n",
      "batch: 1306, loss: 0.5992717742919922\n",
      "batch: 1307, loss: 0.5990270972251892\n",
      "batch: 1308, loss: 0.5987638235092163\n",
      "batch: 1309, loss: 0.5984839200973511\n",
      "batch: 1310, loss: 0.598189115524292\n",
      "batch: 1311, loss: 0.5978808999061584\n",
      "batch: 1312, loss: 0.6982030868530273\n",
      "batch: 1313, loss: 0.799191951751709\n",
      "batch: 1314, loss: 0.6729617714881897\n",
      "batch: 1315, loss: 0.596885621547699\n",
      "batch: 1316, loss: 0.7491179704666138\n",
      "batch: 1317, loss: 0.800133228302002\n",
      "batch: 1318, loss: 0.8002489805221558\n",
      "batch: 1319, loss: 0.8002887964248657\n",
      "batch: 1320, loss: 0.8002603650093079\n",
      "batch: 1321, loss: 0.7747088670730591\n",
      "batch: 1322, loss: 0.5965852737426758\n",
      "batch: 1323, loss: 0.5966383218765259\n",
      "batch: 1324, loss: 0.5966433882713318\n",
      "batch: 1325, loss: 0.5966050624847412\n",
      "batch: 1326, loss: 0.5965278148651123\n",
      "batch: 1327, loss: 0.6983317732810974\n",
      "batch: 1328, loss: 0.8003659844398499\n",
      "batch: 1329, loss: 0.800407886505127\n",
      "batch: 1330, loss: 0.8003811836242676\n",
      "batch: 1331, loss: 0.80029296875\n",
      "batch: 1332, loss: 0.8001490831375122\n",
      "batch: 1333, loss: 0.7999556064605713\n",
      "batch: 1334, loss: 0.7997170090675354\n",
      "batch: 1335, loss: 0.7994384169578552\n",
      "batch: 1336, loss: 0.7991234064102173\n",
      "batch: 1337, loss: 0.7987760305404663\n",
      "batch: 1338, loss: 0.748280942440033\n",
      "batch: 1339, loss: 0.5982317328453064\n",
      "batch: 1340, loss: 0.5984643697738647\n",
      "batch: 1341, loss: 0.5986308455467224\n",
      "batch: 1342, loss: 0.6980729103088379\n",
      "batch: 1343, loss: 0.7972848415374756\n",
      "batch: 1344, loss: 0.7971101403236389\n",
      "batch: 1345, loss: 0.7968889474868774\n",
      "batch: 1346, loss: 0.796626091003418\n",
      "batch: 1347, loss: 0.79632568359375\n",
      "batch: 1348, loss: 0.7959916591644287\n",
      "batch: 1349, loss: 0.7956273555755615\n",
      "batch: 1350, loss: 0.600521981716156\n",
      "batch: 1351, loss: 0.6007686257362366\n",
      "batch: 1352, loss: 0.6009476184844971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1353, loss: 0.6010656356811523\n",
      "batch: 1354, loss: 0.6011285781860352\n",
      "batch: 1355, loss: 0.6011419892311096\n",
      "batch: 1356, loss: 0.6011108756065369\n",
      "batch: 1357, loss: 0.601039469242096\n",
      "batch: 1358, loss: 0.6009321808815002\n",
      "batch: 1359, loss: 0.6007924675941467\n",
      "batch: 1360, loss: 0.600623369216919\n",
      "batch: 1361, loss: 0.6004281640052795\n",
      "batch: 1362, loss: 0.6002092957496643\n",
      "batch: 1363, loss: 0.5999693274497986\n",
      "batch: 1364, loss: 0.5997101664543152\n",
      "batch: 1365, loss: 0.599433958530426\n",
      "batch: 1366, loss: 0.5991424918174744\n",
      "batch: 1367, loss: 0.5988370776176453\n",
      "batch: 1368, loss: 0.5985192656517029\n",
      "batch: 1369, loss: 0.5981904864311218\n",
      "batch: 1370, loss: 0.5978516936302185\n",
      "batch: 1371, loss: 0.5975040793418884\n",
      "batch: 1372, loss: 0.5971483588218689\n",
      "batch: 1373, loss: 0.5967855453491211\n",
      "batch: 1374, loss: 0.5964164137840271\n",
      "batch: 1375, loss: 0.5960415005683899\n",
      "batch: 1376, loss: 0.5956615805625916\n",
      "batch: 1377, loss: 0.5952770709991455\n",
      "batch: 1378, loss: 0.7762190699577332\n",
      "batch: 1379, loss: 0.8025036454200745\n",
      "batch: 1380, loss: 0.5943538546562195\n",
      "batch: 1381, loss: 0.5941082835197449\n",
      "batch: 1382, loss: 0.5938448905944824\n",
      "batch: 1383, loss: 0.5935654044151306\n",
      "batch: 1384, loss: 0.5932713150978088\n",
      "batch: 1385, loss: 0.5929644107818604\n",
      "batch: 1386, loss: 0.5926458239555359\n",
      "batch: 1387, loss: 0.698806643486023\n",
      "batch: 1388, loss: 0.8056564331054688\n",
      "batch: 1389, loss: 0.672103762626648\n",
      "batch: 1390, loss: 0.5916209816932678\n",
      "batch: 1391, loss: 0.7526718378067017\n",
      "batch: 1392, loss: 0.8066359162330627\n",
      "batch: 1393, loss: 0.8067578673362732\n",
      "batch: 1394, loss: 0.8068023920059204\n",
      "batch: 1395, loss: 0.8067775368690491\n",
      "batch: 1396, loss: 0.8066902160644531\n",
      "batch: 1397, loss: 0.8065466284751892\n",
      "batch: 1398, loss: 0.8063524961471558\n",
      "batch: 1399, loss: 0.8061127066612244\n",
      "batch: 1400, loss: 0.7256016731262207\n",
      "batch: 1401, loss: 0.5921047329902649\n",
      "batch: 1402, loss: 0.6988130211830139\n",
      "batch: 1403, loss: 0.8051844835281372\n",
      "batch: 1404, loss: 0.8049573302268982\n",
      "batch: 1405, loss: 0.8046879768371582\n",
      "batch: 1406, loss: 0.645888090133667\n",
      "batch: 1407, loss: 0.593262255191803\n",
      "batch: 1408, loss: 0.7776337265968323\n",
      "batch: 1409, loss: 0.8037441968917847\n",
      "batch: 1410, loss: 0.8034921884536743\n",
      "batch: 1411, loss: 0.8032011389732361\n",
      "batch: 1412, loss: 0.8028745055198669\n",
      "batch: 1413, loss: 0.8025160431861877\n",
      "batch: 1414, loss: 0.8021292090415955\n",
      "batch: 1415, loss: 0.801716685295105\n",
      "batch: 1416, loss: 0.8012810945510864\n",
      "batch: 1417, loss: 0.8008250594139099\n",
      "batch: 1418, loss: 0.800350546836853\n",
      "batch: 1419, loss: 0.7998594641685486\n",
      "batch: 1420, loss: 0.7993535995483398\n",
      "batch: 1421, loss: 0.6478858590126038\n",
      "batch: 1422, loss: 0.5979328751564026\n",
      "batch: 1423, loss: 0.7730650901794434\n",
      "batch: 1424, loss: 0.7976817488670349\n",
      "batch: 1425, loss: 0.5988325476646423\n",
      "batch: 1426, loss: 0.5990768074989319\n",
      "batch: 1427, loss: 0.5992538332939148\n",
      "batch: 1428, loss: 0.599370002746582\n",
      "batch: 1429, loss: 0.5994316935539246\n",
      "batch: 1430, loss: 0.5994439721107483\n",
      "batch: 1431, loss: 0.599412202835083\n",
      "batch: 1432, loss: 0.5993404388427734\n",
      "batch: 1433, loss: 0.5992327332496643\n",
      "batch: 1434, loss: 0.5990942716598511\n",
      "batch: 1435, loss: 0.5989241600036621\n",
      "batch: 1436, loss: 0.5987288951873779\n",
      "batch: 1437, loss: 0.5985111594200134\n",
      "batch: 1438, loss: 0.5982708930969238\n",
      "batch: 1439, loss: 0.598012387752533\n",
      "batch: 1440, loss: 0.5977368950843811\n",
      "batch: 1441, loss: 0.5974461436271667\n",
      "batch: 1442, loss: 0.5971417427062988\n",
      "batch: 1443, loss: 0.5968248844146729\n",
      "batch: 1444, loss: 0.596497118473053\n",
      "batch: 1445, loss: 0.5961594581604004\n",
      "batch: 1446, loss: 0.595812976360321\n",
      "batch: 1447, loss: 0.595458447933197\n",
      "batch: 1448, loss: 0.595097005367279\n",
      "batch: 1449, loss: 0.5947290062904358\n",
      "batch: 1450, loss: 0.594355583190918\n",
      "batch: 1451, loss: 0.5939781069755554\n",
      "batch: 1452, loss: 0.5935937166213989\n",
      "batch: 1453, loss: 0.5932064652442932\n",
      "batch: 1454, loss: 0.5928158760070801\n",
      "batch: 1455, loss: 0.5924219489097595\n",
      "batch: 1456, loss: 0.5920252203941345\n",
      "batch: 1457, loss: 0.5916268825531006\n",
      "batch: 1458, loss: 0.5912250876426697\n",
      "batch: 1459, loss: 0.5908212065696716\n",
      "batch: 1460, loss: 0.5904161334037781\n",
      "batch: 1461, loss: 0.5900096893310547\n",
      "batch: 1462, loss: 0.6991338729858398\n",
      "batch: 1463, loss: 0.8091168999671936\n",
      "batch: 1464, loss: 0.8094573020935059\n",
      "batch: 1465, loss: 0.8096981048583984\n",
      "batch: 1466, loss: 0.8098481297492981\n",
      "batch: 1467, loss: 0.8099209070205688\n",
      "batch: 1468, loss: 0.8099194765090942\n",
      "batch: 1469, loss: 0.8098529577255249\n",
      "batch: 1470, loss: 0.8097277879714966\n",
      "batch: 1471, loss: 0.8095499277114868\n",
      "batch: 1472, loss: 0.8093246817588806\n",
      "batch: 1473, loss: 0.7541146278381348\n",
      "batch: 1474, loss: 0.5895102620124817\n",
      "batch: 1475, loss: 0.589668333530426\n",
      "batch: 1476, loss: 0.5897687077522278\n",
      "batch: 1477, loss: 0.6991075873374939\n",
      "batch: 1478, loss: 0.8083377480506897\n",
      "batch: 1479, loss: 0.8082177639007568\n",
      "batch: 1480, loss: 0.8080456852912903\n",
      "batch: 1481, loss: 0.8078252077102661\n",
      "batch: 1482, loss: 0.8075618743896484\n",
      "batch: 1483, loss: 0.6177984476089478\n",
      "batch: 1484, loss: 0.59092116355896\n",
      "batch: 1485, loss: 0.8068675398826599\n",
      "batch: 1486, loss: 0.8066601157188416\n",
      "batch: 1487, loss: 0.8064087629318237\n",
      "batch: 1488, loss: 0.806117594242096\n",
      "batch: 1489, loss: 0.8057907819747925\n",
      "batch: 1490, loss: 0.8054322004318237\n",
      "batch: 1491, loss: 0.8050447106361389\n",
      "batch: 1492, loss: 0.8046313524246216\n",
      "batch: 1493, loss: 0.8041952848434448\n",
      "batch: 1494, loss: 0.8037381172180176\n",
      "batch: 1495, loss: 0.8032625913619995\n",
      "batch: 1496, loss: 0.80277019739151\n",
      "batch: 1497, loss: 0.8022631406784058\n",
      "batch: 1498, loss: 0.8017425537109375\n",
      "batch: 1499, loss: 0.8012100458145142\n",
      "batch: 1500, loss: 0.5960791110992432\n",
      "batch: 1501, loss: 0.5964306592941284\n",
      "batch: 1502, loss: 0.5967063307762146\n",
      "batch: 1503, loss: 0.647594690322876\n",
      "batch: 1504, loss: 0.7994290590286255\n",
      "batch: 1505, loss: 0.7234678268432617\n",
      "batch: 1506, loss: 0.5975041389465332\n",
      "batch: 1507, loss: 0.5976481437683105\n",
      "batch: 1508, loss: 0.5977036952972412\n",
      "batch: 1509, loss: 0.597737193107605\n",
      "batch: 1510, loss: 0.5977264642715454\n",
      "batch: 1511, loss: 0.597662091255188\n",
      "batch: 1512, loss: 0.5975663661956787\n",
      "batch: 1513, loss: 0.5974467396736145\n",
      "batch: 1514, loss: 0.5972893238067627\n",
      "batch: 1515, loss: 0.5970999002456665\n",
      "batch: 1516, loss: 0.596885085105896\n",
      "batch: 1517, loss: 0.5966512560844421\n",
      "batch: 1518, loss: 0.5964028835296631\n",
      "batch: 1519, loss: 0.5961305499076843\n",
      "batch: 1520, loss: 0.6727588176727295\n",
      "batch: 1521, loss: 0.8012689352035522\n",
      "batch: 1522, loss: 0.8014965057373047\n",
      "batch: 1523, loss: 0.8016363382339478\n",
      "batch: 1524, loss: 0.6726573705673218\n",
      "batch: 1525, loss: 0.5951836705207825\n",
      "batch: 1526, loss: 0.5950949788093567\n",
      "batch: 1527, loss: 0.5949711799621582\n",
      "batch: 1528, loss: 0.5948195457458496\n",
      "batch: 1529, loss: 0.5946382284164429\n",
      "batch: 1530, loss: 0.5944307446479797\n",
      "batch: 1531, loss: 0.5942029356956482\n",
      "batch: 1532, loss: 0.593951940536499\n",
      "batch: 1533, loss: 0.5936837196350098\n",
      "batch: 1534, loss: 0.5934000015258789\n",
      "batch: 1535, loss: 0.6723114848136902\n",
      "batch: 1536, loss: 0.8046717643737793\n",
      "batch: 1537, loss: 0.804914116859436\n",
      "batch: 1538, loss: 0.8050649762153625\n",
      "batch: 1539, loss: 0.8051290512084961\n",
      "batch: 1540, loss: 0.8051263093948364\n",
      "batch: 1541, loss: 0.8050533533096313\n",
      "batch: 1542, loss: 0.8049205541610718\n",
      "batch: 1543, loss: 0.8047367930412292\n",
      "batch: 1544, loss: 0.8045050501823425\n",
      "batch: 1545, loss: 0.8042323589324951\n",
      "batch: 1546, loss: 0.8039229512214661\n",
      "batch: 1547, loss: 0.8035765290260315\n",
      "batch: 1548, loss: 0.8031964302062988\n",
      "batch: 1549, loss: 0.802784264087677\n",
      "batch: 1550, loss: 0.8023309707641602\n",
      "batch: 1551, loss: 0.8018333911895752\n",
      "batch: 1552, loss: 0.8013244867324829\n",
      "batch: 1553, loss: 0.8007770776748657\n",
      "batch: 1554, loss: 0.8001916408538818\n",
      "batch: 1555, loss: 0.7995820641517639\n",
      "batch: 1556, loss: 0.6478492617607117\n",
      "batch: 1557, loss: 0.5979546308517456\n",
      "batch: 1558, loss: 0.5983176827430725\n",
      "batch: 1559, loss: 0.5985386967658997\n",
      "batch: 1560, loss: 0.598666250705719\n",
      "batch: 1561, loss: 0.6235604286193848\n",
      "batch: 1562, loss: 0.7974224090576172\n",
      "batch: 1563, loss: 0.7973306775093079\n",
      "batch: 1564, loss: 0.7971336245536804\n",
      "batch: 1565, loss: 0.7227309942245483\n",
      "batch: 1566, loss: 0.59942227602005\n",
      "batch: 1567, loss: 0.6979647278785706\n",
      "batch: 1568, loss: 0.7962246537208557\n",
      "batch: 1569, loss: 0.7959691882133484\n",
      "batch: 1570, loss: 0.7956046462059021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1571, loss: 0.6492304801940918\n",
      "batch: 1572, loss: 0.6009083986282349\n",
      "batch: 1573, loss: 0.7703667879104614\n",
      "batch: 1574, loss: 0.7942020893096924\n",
      "batch: 1575, loss: 0.60130375623703\n",
      "batch: 1576, loss: 0.6014633178710938\n",
      "batch: 1577, loss: 0.6015539169311523\n",
      "batch: 1578, loss: 0.6015347838401794\n",
      "batch: 1579, loss: 0.601452112197876\n",
      "batch: 1580, loss: 0.6012139916419983\n",
      "batch: 1581, loss: 0.6009461283683777\n",
      "batch: 1582, loss: 0.6006637811660767\n",
      "batch: 1583, loss: 0.6003378629684448\n",
      "batch: 1584, loss: 0.7224608659744263\n",
      "batch: 1585, loss: 0.7962337136268616\n",
      "batch: 1586, loss: 0.6487724184989929\n",
      "batch: 1587, loss: 0.599428117275238\n",
      "batch: 1588, loss: 0.5992893576622009\n",
      "batch: 1589, loss: 0.5991228818893433\n",
      "batch: 1590, loss: 0.5989335775375366\n",
      "batch: 1591, loss: 0.6235576868057251\n",
      "batch: 1592, loss: 0.797702431678772\n",
      "batch: 1593, loss: 0.7479991316795349\n",
      "batch: 1594, loss: 0.5982444882392883\n",
      "batch: 1595, loss: 0.6731336116790771\n",
      "batch: 1596, loss: 0.7983359694480896\n",
      "batch: 1597, loss: 0.6981639862060547\n",
      "batch: 1598, loss: 0.5978592038154602\n",
      "batch: 1599, loss: 0.5977680683135986\n",
      "batch: 1600, loss: 0.5976431965827942\n",
      "batch: 1601, loss: 0.748572826385498\n",
      "batch: 1602, loss: 0.7990704774856567\n",
      "batch: 1603, loss: 0.6225540637969971\n",
      "batch: 1604, loss: 0.597255289554596\n",
      "batch: 1605, loss: 0.5971460342407227\n",
      "batch: 1606, loss: 0.5970048904418945\n",
      "batch: 1607, loss: 0.5968350768089294\n",
      "batch: 1608, loss: 0.5966392159461975\n",
      "batch: 1609, loss: 0.5964203476905823\n",
      "batch: 1610, loss: 0.5961806178092957\n",
      "batch: 1611, loss: 0.59592205286026\n",
      "batch: 1612, loss: 0.6984189748764038\n",
      "batch: 1613, loss: 0.8014897704124451\n",
      "batch: 1614, loss: 0.8016939163208008\n",
      "batch: 1615, loss: 0.8018130660057068\n",
      "batch: 1616, loss: 0.8018559813499451\n",
      "batch: 1617, loss: 0.8018299341201782\n",
      "batch: 1618, loss: 0.8017420172691345\n",
      "batch: 1619, loss: 0.8015984892845154\n",
      "batch: 1620, loss: 0.8014047145843506\n",
      "batch: 1621, loss: 0.8011661767959595\n",
      "batch: 1622, loss: 0.8008872270584106\n",
      "batch: 1623, loss: 0.7494665384292603\n",
      "batch: 1624, loss: 0.596411406993866\n",
      "batch: 1625, loss: 0.6728835701942444\n",
      "batch: 1626, loss: 0.7998155951499939\n",
      "batch: 1627, loss: 0.7995692491531372\n",
      "batch: 1628, loss: 0.7992830872535706\n",
      "batch: 1629, loss: 0.7989615797996521\n",
      "batch: 1630, loss: 0.7986080050468445\n",
      "batch: 1631, loss: 0.7982258796691895\n",
      "batch: 1632, loss: 0.7978176474571228\n",
      "batch: 1633, loss: 0.7973865270614624\n",
      "batch: 1634, loss: 0.7969344854354858\n",
      "batch: 1635, loss: 0.7964634895324707\n",
      "batch: 1636, loss: 0.7714678645133972\n",
      "batch: 1637, loss: 0.6003151535987854\n",
      "batch: 1638, loss: 0.6492500901222229\n",
      "batch: 1639, loss: 0.7947758436203003\n",
      "batch: 1640, loss: 0.794419527053833\n",
      "batch: 1641, loss: 0.7940338850021362\n",
      "batch: 1642, loss: 0.7936208248138428\n",
      "batch: 1643, loss: 0.7931892275810242\n",
      "batch: 1644, loss: 0.7927308082580566\n",
      "batch: 1645, loss: 0.7922517657279968\n",
      "batch: 1646, loss: 0.7917560338973999\n",
      "batch: 1647, loss: 0.7912459373474121\n",
      "batch: 1648, loss: 0.7907118797302246\n",
      "batch: 1649, loss: 0.7901632189750671\n",
      "batch: 1650, loss: 0.6051855683326721\n",
      "batch: 1651, loss: 0.6055484414100647\n",
      "batch: 1652, loss: 0.6058244705200195\n",
      "batch: 1653, loss: 0.6060417294502258\n",
      "batch: 1654, loss: 0.6061776280403137\n",
      "batch: 1655, loss: 0.6062421798706055\n",
      "batch: 1656, loss: 0.6062626838684082\n",
      "batch: 1657, loss: 0.6062260270118713\n",
      "batch: 1658, loss: 0.6061545610427856\n",
      "batch: 1659, loss: 0.6060423851013184\n",
      "batch: 1660, loss: 0.6058972477912903\n",
      "batch: 1661, loss: 0.6057161092758179\n",
      "batch: 1662, loss: 0.6055207252502441\n",
      "batch: 1663, loss: 0.6053067445755005\n",
      "batch: 1664, loss: 0.605070948600769\n",
      "batch: 1665, loss: 0.6048117280006409\n",
      "batch: 1666, loss: 0.6045371294021606\n",
      "batch: 1667, loss: 0.6042509078979492\n",
      "batch: 1668, loss: 0.6039478182792664\n",
      "batch: 1669, loss: 0.6036273241043091\n",
      "batch: 1670, loss: 0.603295624256134\n",
      "batch: 1671, loss: 0.6029497385025024\n",
      "batch: 1672, loss: 0.6026027202606201\n",
      "batch: 1673, loss: 0.6022379398345947\n",
      "batch: 1674, loss: 0.6018704771995544\n",
      "batch: 1675, loss: 0.6014953851699829\n",
      "batch: 1676, loss: 0.6011060476303101\n",
      "batch: 1677, loss: 0.6007200479507446\n",
      "batch: 1678, loss: 0.6003236770629883\n",
      "batch: 1679, loss: 0.5999152064323425\n",
      "batch: 1680, loss: 0.7964833974838257\n",
      "batch: 1681, loss: 0.7721548080444336\n",
      "batch: 1682, loss: 0.5989322662353516\n",
      "batch: 1683, loss: 0.5986796021461487\n",
      "batch: 1684, loss: 0.5984019637107849\n",
      "batch: 1685, loss: 0.6731259822845459\n",
      "batch: 1686, loss: 0.7985542416572571\n",
      "batch: 1687, loss: 0.7987850308418274\n",
      "batch: 1688, loss: 0.798923909664154\n",
      "batch: 1689, loss: 0.7989673614501953\n",
      "batch: 1690, loss: 0.7989746332168579\n",
      "batch: 1691, loss: 0.7988614439964294\n",
      "batch: 1692, loss: 0.7986891269683838\n",
      "batch: 1693, loss: 0.6229309439659119\n",
      "batch: 1694, loss: 0.5979734063148499\n",
      "batch: 1695, loss: 0.798269510269165\n",
      "batch: 1696, loss: 0.7731373310089111\n",
      "batch: 1697, loss: 0.5982705354690552\n",
      "batch: 1698, loss: 0.5983582139015198\n",
      "batch: 1699, loss: 0.5983952283859253\n",
      "batch: 1700, loss: 0.5983771085739136\n",
      "batch: 1701, loss: 0.5983110070228577\n",
      "batch: 1702, loss: 0.6981303095817566\n",
      "batch: 1703, loss: 0.7981529831886292\n",
      "batch: 1704, loss: 0.7981950044631958\n",
      "batch: 1705, loss: 0.798161506652832\n",
      "batch: 1706, loss: 0.7980325818061829\n",
      "batch: 1707, loss: 0.7978852987289429\n",
      "batch: 1708, loss: 0.6234080195426941\n",
      "batch: 1709, loss: 0.5986324548721313\n",
      "batch: 1710, loss: 0.5986932516098022\n",
      "batch: 1711, loss: 0.5987035036087036\n",
      "batch: 1712, loss: 0.5986776947975159\n",
      "batch: 1713, loss: 0.5986047387123108\n",
      "batch: 1714, loss: 0.5984973907470703\n",
      "batch: 1715, loss: 0.6731714010238647\n",
      "batch: 1716, loss: 0.7980304956436157\n",
      "batch: 1717, loss: 0.7981168031692505\n",
      "batch: 1718, loss: 0.7981241941452026\n",
      "batch: 1719, loss: 0.7980664372444153\n",
      "batch: 1720, loss: 0.7979483008384705\n",
      "batch: 1721, loss: 0.7977830171585083\n",
      "batch: 1722, loss: 0.7975717782974243\n",
      "batch: 1723, loss: 0.7973122000694275\n",
      "batch: 1724, loss: 0.7970187067985535\n",
      "batch: 1725, loss: 0.5993313193321228\n",
      "batch: 1726, loss: 0.5995309352874756\n",
      "batch: 1727, loss: 0.5996675491333008\n",
      "batch: 1728, loss: 0.5997476577758789\n",
      "batch: 1729, loss: 0.5997767448425293\n",
      "batch: 1730, loss: 0.5997598171234131\n",
      "batch: 1731, loss: 0.5997015833854675\n",
      "batch: 1732, loss: 0.5996063351631165\n",
      "batch: 1733, loss: 0.5994775295257568\n",
      "batch: 1734, loss: 0.5993185639381409\n",
      "batch: 1735, loss: 0.5991325974464417\n",
      "batch: 1736, loss: 0.7476176619529724\n",
      "batch: 1737, loss: 0.7973788976669312\n",
      "batch: 1738, loss: 0.6235220432281494\n",
      "batch: 1739, loss: 0.598555326461792\n",
      "batch: 1740, loss: 0.5984097123146057\n",
      "batch: 1741, loss: 0.5982358455657959\n",
      "batch: 1742, loss: 0.598036527633667\n",
      "batch: 1743, loss: 0.5978142619132996\n",
      "batch: 1744, loss: 0.5975714325904846\n",
      "batch: 1745, loss: 0.5973101258277893\n",
      "batch: 1746, loss: 0.597032368183136\n",
      "batch: 1747, loss: 0.5967395901679993\n",
      "batch: 1748, loss: 0.5964334607124329\n",
      "batch: 1749, loss: 0.5961153507232666\n",
      "batch: 1750, loss: 0.5957865118980408\n",
      "batch: 1751, loss: 0.5954479575157166\n",
      "batch: 1752, loss: 0.5951008200645447\n",
      "batch: 1753, loss: 0.7763546705245972\n",
      "batch: 1754, loss: 0.8026419878005981\n",
      "batch: 1755, loss: 0.8028864860534668\n",
      "batch: 1756, loss: 0.7769296765327454\n",
      "batch: 1757, loss: 0.5940691828727722\n",
      "batch: 1758, loss: 0.5939609408378601\n",
      "batch: 1759, loss: 0.5938210487365723\n",
      "batch: 1760, loss: 0.6724001169204712\n",
      "batch: 1761, loss: 0.8038411736488342\n",
      "batch: 1762, loss: 0.8039525747299194\n",
      "batch: 1763, loss: 0.803987979888916\n",
      "batch: 1764, loss: 0.8039554953575134\n",
      "batch: 1765, loss: 0.8038615584373474\n",
      "batch: 1766, loss: 0.8037124276161194\n",
      "batch: 1767, loss: 0.8035139441490173\n",
      "batch: 1768, loss: 0.8032704591751099\n",
      "batch: 1769, loss: 0.8029872179031372\n",
      "batch: 1770, loss: 0.802668035030365\n",
      "batch: 1771, loss: 0.8023165464401245\n",
      "batch: 1772, loss: 0.8019360303878784\n",
      "batch: 1773, loss: 0.7499898672103882\n",
      "batch: 1774, loss: 0.5956980586051941\n",
      "batch: 1775, loss: 0.6727757453918457\n",
      "batch: 1776, loss: 0.8005504608154297\n",
      "batch: 1777, loss: 0.8002442121505737\n",
      "batch: 1778, loss: 0.7999044060707092\n",
      "batch: 1779, loss: 0.799534797668457\n",
      "batch: 1780, loss: 0.7991381287574768\n",
      "batch: 1781, loss: 0.7987174987792969\n",
      "batch: 1782, loss: 0.7982751131057739\n",
      "batch: 1783, loss: 0.7978134155273438\n",
      "batch: 1784, loss: 0.7973342537879944\n",
      "batch: 1785, loss: 0.5992040038108826\n",
      "batch: 1786, loss: 0.6241415143013\n",
      "batch: 1787, loss: 0.7961310744285583\n",
      "batch: 1788, loss: 0.795783281326294\n",
      "batch: 1789, loss: 0.7954068779945374\n",
      "batch: 1790, loss: 0.7950048446655273\n",
      "batch: 1791, loss: 0.7945796251296997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1792, loss: 0.7941337823867798\n",
      "batch: 1793, loss: 0.7936694622039795\n",
      "batch: 1794, loss: 0.7931884527206421\n",
      "batch: 1795, loss: 0.7926924228668213\n",
      "batch: 1796, loss: 0.7921832799911499\n",
      "batch: 1797, loss: 0.7916620969772339\n",
      "batch: 1798, loss: 0.7911301255226135\n",
      "batch: 1799, loss: 0.790588915348053\n",
      "batch: 1800, loss: 0.6048195958137512\n",
      "batch: 1801, loss: 0.6282384395599365\n",
      "batch: 1802, loss: 0.7892364859580994\n",
      "batch: 1803, loss: 0.7430894374847412\n",
      "batch: 1804, loss: 0.6061274409294128\n",
      "batch: 1805, loss: 0.6745474934577942\n",
      "batch: 1806, loss: 0.7879223823547363\n",
      "batch: 1807, loss: 0.697227954864502\n",
      "batch: 1808, loss: 0.6070470213890076\n",
      "batch: 1809, loss: 0.7196871042251587\n",
      "batch: 1810, loss: 0.7869886159896851\n",
      "batch: 1811, loss: 0.6523555517196655\n",
      "batch: 1812, loss: 0.607710599899292\n",
      "batch: 1813, loss: 0.6078064441680908\n",
      "batch: 1814, loss: 0.6078488826751709\n",
      "batch: 1815, loss: 0.6078432202339172\n",
      "batch: 1816, loss: 0.6301289200782776\n",
      "batch: 1817, loss: 0.7865614295005798\n",
      "batch: 1818, loss: 0.7418608665466309\n",
      "batch: 1819, loss: 0.607716977596283\n",
      "batch: 1820, loss: 0.6076857447624207\n",
      "batch: 1821, loss: 0.6076140999794006\n",
      "batch: 1822, loss: 0.6075053811073303\n",
      "batch: 1823, loss: 0.6073641180992126\n",
      "batch: 1824, loss: 0.6071929931640625\n",
      "batch: 1825, loss: 0.6069953441619873\n",
      "batch: 1826, loss: 0.6067737340927124\n",
      "batch: 1827, loss: 0.6065306067466736\n",
      "batch: 1828, loss: 0.606268048286438\n",
      "batch: 1829, loss: 0.6059882640838623\n",
      "batch: 1830, loss: 0.6056928038597107\n",
      "batch: 1831, loss: 0.628380537033081\n",
      "batch: 1832, loss: 0.78973388671875\n",
      "batch: 1833, loss: 0.7437164783477783\n",
      "batch: 1834, loss: 0.6046714186668396\n",
      "batch: 1835, loss: 0.6044698357582092\n",
      "batch: 1836, loss: 0.6042451858520508\n",
      "batch: 1837, loss: 0.6039994359016418\n",
      "batch: 1838, loss: 0.6037349104881287\n",
      "batch: 1839, loss: 0.6034533977508545\n",
      "batch: 1840, loss: 0.6031569242477417\n",
      "batch: 1841, loss: 0.7450257539749146\n",
      "batch: 1842, loss: 0.7927221059799194\n",
      "batch: 1843, loss: 0.7929316163063049\n",
      "batch: 1844, loss: 0.7930569052696228\n",
      "batch: 1845, loss: 0.7931060194969177\n",
      "batch: 1846, loss: 0.7930869460105896\n",
      "batch: 1847, loss: 0.7930063605308533\n",
      "batch: 1848, loss: 0.7452711462974548\n",
      "batch: 1849, loss: 0.6026026606559753\n",
      "batch: 1850, loss: 0.6026759147644043\n",
      "batch: 1851, loss: 0.602698564529419\n",
      "batch: 1852, loss: 0.6976504921913147\n",
      "batch: 1853, loss: 0.7926449179649353\n",
      "batch: 1854, loss: 0.792599081993103\n",
      "batch: 1855, loss: 0.7924942970275879\n",
      "batch: 1856, loss: 0.7923369407653809\n",
      "batch: 1857, loss: 0.7921319007873535\n",
      "batch: 1858, loss: 0.791884183883667\n",
      "batch: 1859, loss: 0.7915982604026794\n",
      "batch: 1860, loss: 0.7912776470184326\n",
      "batch: 1861, loss: 0.7909260988235474\n",
      "batch: 1862, loss: 0.7905468344688416\n",
      "batch: 1863, loss: 0.7901424765586853\n",
      "batch: 1864, loss: 0.7897158265113831\n",
      "batch: 1865, loss: 0.7892690300941467\n",
      "batch: 1866, loss: 0.7888044118881226\n",
      "batch: 1867, loss: 0.7883234024047852\n",
      "batch: 1868, loss: 0.7878282070159912\n",
      "batch: 1869, loss: 0.787320077419281\n",
      "batch: 1870, loss: 0.7868003249168396\n",
      "batch: 1871, loss: 0.7862701416015625\n",
      "batch: 1872, loss: 0.7857308387756348\n",
      "batch: 1873, loss: 0.7851833701133728\n",
      "batch: 1874, loss: 0.7846284508705139\n",
      "batch: 1875, loss: 0.6098089814186096\n",
      "batch: 1876, loss: 0.6101894974708557\n",
      "batch: 1877, loss: 0.6104881763458252\n",
      "batch: 1878, loss: 0.6107129454612732\n",
      "batch: 1879, loss: 0.6108712553977966\n",
      "batch: 1880, loss: 0.6109696626663208\n",
      "batch: 1881, loss: 0.6110143065452576\n",
      "batch: 1882, loss: 0.6968246698379517\n",
      "batch: 1883, loss: 0.7826385498046875\n",
      "batch: 1884, loss: 0.6753806471824646\n",
      "batch: 1885, loss: 0.6111032962799072\n",
      "batch: 1886, loss: 0.6110947728157043\n",
      "batch: 1887, loss: 0.6110430359840393\n",
      "batch: 1888, loss: 0.6109524369239807\n",
      "batch: 1889, loss: 0.6108267903327942\n",
      "batch: 1890, loss: 0.6106694936752319\n",
      "batch: 1891, loss: 0.6320813894271851\n",
      "batch: 1892, loss: 0.7835010886192322\n",
      "batch: 1893, loss: 0.740278422832489\n",
      "batch: 1894, loss: 0.6100724339485168\n",
      "batch: 1895, loss: 0.6099511384963989\n",
      "batch: 1896, loss: 0.6097981333732605\n",
      "batch: 1897, loss: 0.6096162796020508\n",
      "batch: 1898, loss: 0.6094086170196533\n",
      "batch: 1899, loss: 0.6091777086257935\n",
      "batch: 1900, loss: 0.6089261770248413\n",
      "batch: 1901, loss: 0.6086549758911133\n",
      "batch: 1902, loss: 0.608367383480072\n",
      "batch: 1903, loss: 0.6080613732337952\n",
      "batch: 1904, loss: 0.607743501663208\n",
      "batch: 1905, loss: 0.6074161529541016\n",
      "batch: 1906, loss: 0.6070734262466431\n",
      "batch: 1907, loss: 0.6067175269126892\n",
      "batch: 1908, loss: 0.606353759765625\n",
      "batch: 1909, loss: 0.6059824228286743\n",
      "batch: 1910, loss: 0.6056064367294312\n",
      "batch: 1911, loss: 0.6052132844924927\n",
      "batch: 1912, loss: 0.6048170924186707\n",
      "batch: 1913, loss: 0.6044124960899353\n",
      "batch: 1914, loss: 0.6040046215057373\n",
      "batch: 1915, loss: 0.603577733039856\n",
      "batch: 1916, loss: 0.7448320984840393\n",
      "batch: 1917, loss: 0.7924855351448059\n",
      "batch: 1918, loss: 0.62632155418396\n",
      "batch: 1919, loss: 0.6022710204124451\n",
      "batch: 1920, loss: 0.6019641160964966\n",
      "batch: 1921, loss: 0.6016344428062439\n",
      "batch: 1922, loss: 0.6012977361679077\n",
      "batch: 1923, loss: 0.6009066700935364\n",
      "batch: 1924, loss: 0.6004983186721802\n",
      "batch: 1925, loss: 0.6000486016273499\n",
      "batch: 1926, loss: 0.5995690226554871\n",
      "batch: 1927, loss: 0.59906005859375\n",
      "batch: 1928, loss: 0.5985119938850403\n",
      "batch: 1929, loss: 0.7232658267021179\n",
      "batch: 1930, loss: 0.7991085052490234\n",
      "batch: 1931, loss: 0.7995400428771973\n",
      "batch: 1932, loss: 0.79975426197052\n",
      "batch: 1933, loss: 0.7997874021530151\n",
      "batch: 1934, loss: 0.799616813659668\n",
      "batch: 1935, loss: 0.7993279099464417\n",
      "batch: 1936, loss: 0.7989941835403442\n",
      "batch: 1937, loss: 0.7986085414886475\n",
      "batch: 1938, loss: 0.7981632947921753\n",
      "batch: 1939, loss: 0.7976978421211243\n",
      "batch: 1940, loss: 0.7972155809402466\n",
      "batch: 1941, loss: 0.7967549562454224\n",
      "batch: 1942, loss: 0.7962799668312073\n",
      "batch: 1943, loss: 0.7958123683929443\n",
      "batch: 1944, loss: 0.7953521013259888\n",
      "batch: 1945, loss: 0.7948929667472839\n",
      "batch: 1946, loss: 0.7944342494010925\n",
      "batch: 1947, loss: 0.7939669489860535\n",
      "batch: 1948, loss: 0.7934895157814026\n",
      "batch: 1949, loss: 0.7929943203926086\n",
      "batch: 1950, loss: 0.6027642488479614\n",
      "batch: 1951, loss: 0.603090226650238\n",
      "batch: 1952, loss: 0.603344202041626\n",
      "batch: 1953, loss: 0.603522777557373\n",
      "batch: 1954, loss: 0.6036509871482849\n",
      "batch: 1955, loss: 0.6037231087684631\n",
      "batch: 1956, loss: 0.6037447452545166\n",
      "batch: 1957, loss: 0.6975419521331787\n",
      "batch: 1958, loss: 0.791383683681488\n",
      "batch: 1959, loss: 0.6740901470184326\n",
      "batch: 1960, loss: 0.6037664413452148\n",
      "batch: 1961, loss: 0.7444357872009277\n",
      "batch: 1962, loss: 0.7913198471069336\n",
      "batch: 1963, loss: 0.6272465586662292\n",
      "batch: 1964, loss: 0.6038420796394348\n",
      "batch: 1965, loss: 0.6038203835487366\n",
      "batch: 1966, loss: 0.6037573218345642\n",
      "batch: 1967, loss: 0.6036571860313416\n",
      "batch: 1968, loss: 0.6035237312316895\n",
      "batch: 1969, loss: 0.6033602356910706\n",
      "batch: 1970, loss: 0.6739916801452637\n",
      "batch: 1971, loss: 0.7922444343566895\n",
      "batch: 1972, loss: 0.697628915309906\n",
      "batch: 1973, loss: 0.6027889251708984\n",
      "batch: 1974, loss: 0.6026617288589478\n",
      "batch: 1975, loss: 0.6025038361549377\n",
      "batch: 1976, loss: 0.602318525314331\n",
      "batch: 1977, loss: 0.6021084785461426\n",
      "batch: 1978, loss: 0.6018762588500977\n",
      "batch: 1979, loss: 0.601624071598053\n",
      "batch: 1980, loss: 0.6013537049293518\n",
      "batch: 1981, loss: 0.6010674238204956\n",
      "batch: 1982, loss: 0.600766658782959\n",
      "batch: 1983, loss: 0.6004529595375061\n",
      "batch: 1984, loss: 0.6001276969909668\n",
      "batch: 1985, loss: 0.599791944026947\n",
      "batch: 1986, loss: 0.599446713924408\n",
      "batch: 1987, loss: 0.5990932583808899\n",
      "batch: 1988, loss: 0.5987322926521301\n",
      "batch: 1989, loss: 0.5983647704124451\n",
      "batch: 1990, loss: 0.5979909300804138\n",
      "batch: 1991, loss: 0.5976119637489319\n",
      "batch: 1992, loss: 0.5972280502319336\n",
      "batch: 1993, loss: 0.596839964389801\n",
      "batch: 1994, loss: 0.5964481830596924\n",
      "batch: 1995, loss: 0.8006925582885742\n",
      "batch: 1996, loss: 0.7754002213478088\n",
      "batch: 1997, loss: 0.595517635345459\n",
      "batch: 1998, loss: 0.6468642950057983\n",
      "batch: 1999, loss: 0.801960825920105\n",
      "batch: 2000, loss: 0.7244246006011963\n",
      "batch: 2001, loss: 0.5947175025939941\n",
      "batch: 2002, loss: 0.5945537090301514\n",
      "batch: 2003, loss: 0.5943637490272522\n",
      "batch: 2004, loss: 0.5941503047943115\n",
      "batch: 2005, loss: 0.5939157009124756\n",
      "batch: 2006, loss: 0.5936622023582458\n",
      "batch: 2007, loss: 0.5933915972709656\n",
      "batch: 2008, loss: 0.5931057333946228\n",
      "batch: 2009, loss: 0.5928062200546265\n",
      "batch: 2010, loss: 0.5924941897392273\n",
      "batch: 2011, loss: 0.6188343167304993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 2012, loss: 0.8058739900588989\n",
      "batch: 2013, loss: 0.8061670660972595\n",
      "batch: 2014, loss: 0.8063659071922302\n",
      "batch: 2015, loss: 0.7258104681968689\n",
      "batch: 2016, loss: 0.5912953019142151\n",
      "batch: 2017, loss: 0.6989406943321228\n",
      "batch: 2018, loss: 0.8067946434020996\n",
      "batch: 2019, loss: 0.8068260550498962\n",
      "batch: 2020, loss: 0.8067893385887146\n",
      "batch: 2021, loss: 0.8066912293434143\n",
      "batch: 2022, loss: 0.8065381050109863\n",
      "batch: 2023, loss: 0.6183350682258606\n",
      "batch: 2024, loss: 0.5915946364402771\n",
      "batch: 2025, loss: 0.5916574001312256\n",
      "batch: 2026, loss: 0.5916717052459717\n",
      "batch: 2027, loss: 0.5916423797607422\n",
      "batch: 2028, loss: 0.5915736556053162\n",
      "batch: 2029, loss: 0.5914695858955383\n",
      "batch: 2030, loss: 0.5913336277008057\n",
      "batch: 2031, loss: 0.5911691188812256\n",
      "batch: 2032, loss: 0.5909788012504578\n",
      "batch: 2033, loss: 0.5907654166221619\n",
      "batch: 2034, loss: 0.5905311703681946\n",
      "batch: 2035, loss: 0.5902782082557678\n",
      "batch: 2036, loss: 0.590008556842804\n",
      "batch: 2037, loss: 0.5897237658500671\n",
      "batch: 2038, loss: 0.5894255042076111\n",
      "batch: 2039, loss: 0.5891150832176208\n",
      "batch: 2040, loss: 0.5887937545776367\n",
      "batch: 2041, loss: 0.588462769985199\n",
      "batch: 2042, loss: 0.5881229043006897\n",
      "batch: 2043, loss: 0.5877752304077148\n",
      "batch: 2044, loss: 0.5874205231666565\n",
      "batch: 2045, loss: 0.587059736251831\n",
      "batch: 2046, loss: 0.586693286895752\n",
      "batch: 2047, loss: 0.6995430588722229\n",
      "batch: 2048, loss: 0.8131771683692932\n",
      "batch: 2049, loss: 0.6711493134498596\n",
      "batch: 2050, loss: 0.5855233669281006\n",
      "batch: 2051, loss: 0.5852785110473633\n",
      "batch: 2052, loss: 0.585016667842865\n",
      "batch: 2053, loss: 0.5847392678260803\n",
      "batch: 2054, loss: 0.5844480991363525\n",
      "batch: 2055, loss: 0.5841445326805115\n",
      "batch: 2056, loss: 0.583829939365387\n",
      "batch: 2057, loss: 0.5835052132606506\n",
      "batch: 2058, loss: 0.5831717848777771\n",
      "batch: 2059, loss: 0.5828301906585693\n",
      "batch: 2060, loss: 0.5824814438819885\n",
      "batch: 2061, loss: 0.5821263790130615\n",
      "batch: 2062, loss: 0.7001367211341858\n",
      "batch: 2063, loss: 0.8189126253128052\n",
      "batch: 2064, loss: 0.6704610586166382\n",
      "batch: 2065, loss: 0.5809938907623291\n",
      "batch: 2066, loss: 0.7600298523902893\n",
      "batch: 2067, loss: 0.8200216889381409\n",
      "batch: 2068, loss: 0.820165753364563\n",
      "batch: 2069, loss: 0.8202289938926697\n",
      "batch: 2070, loss: 0.820219099521637\n",
      "batch: 2071, loss: 0.8201436996459961\n",
      "batch: 2072, loss: 0.8200093507766724\n",
      "batch: 2073, loss: 0.7600489258766174\n",
      "batch: 2074, loss: 0.5808915495872498\n",
      "batch: 2075, loss: 0.6704290509223938\n",
      "batch: 2076, loss: 0.8193725347518921\n",
      "batch: 2077, loss: 0.819205641746521\n",
      "batch: 2078, loss: 0.8189889192581177\n",
      "batch: 2079, loss: 0.8187278509140015\n",
      "batch: 2080, loss: 0.818426251411438\n",
      "batch: 2081, loss: 0.6410945057868958\n",
      "batch: 2082, loss: 0.5823183655738831\n",
      "batch: 2083, loss: 0.7882160544395447\n",
      "batch: 2084, loss: 0.8173753619194031\n",
      "batch: 2085, loss: 0.5828776359558105\n",
      "batch: 2086, loss: 0.583031952381134\n",
      "batch: 2087, loss: 0.583129346370697\n",
      "batch: 2088, loss: 0.6415627002716064\n",
      "batch: 2089, loss: 0.8166943788528442\n",
      "batch: 2090, loss: 0.8166013360023499\n",
      "batch: 2091, loss: 0.8164515495300293\n",
      "batch: 2092, loss: 0.8162506818771362\n",
      "batch: 2093, loss: 0.8160037398338318\n",
      "batch: 2094, loss: 0.8157156705856323\n",
      "batch: 2095, loss: 0.8153901696205139\n",
      "batch: 2096, loss: 0.8150315284729004\n",
      "batch: 2097, loss: 0.8146430253982544\n",
      "batch: 2098, loss: 0.6137903928756714\n",
      "batch: 2099, loss: 0.5854238867759705\n",
      "batch: 2100, loss: 0.585623025894165\n",
      "batch: 2101, loss: 0.5857605338096619\n",
      "batch: 2102, loss: 0.5858426094055176\n",
      "batch: 2103, loss: 0.5858747959136963\n",
      "batch: 2104, loss: 0.5858621001243591\n",
      "batch: 2105, loss: 0.5858086943626404\n",
      "batch: 2106, loss: 0.585719108581543\n",
      "batch: 2107, loss: 0.5855965614318848\n",
      "batch: 2108, loss: 0.5854447484016418\n",
      "batch: 2109, loss: 0.5852662324905396\n",
      "batch: 2110, loss: 0.585064172744751\n",
      "batch: 2111, loss: 0.5848404169082642\n",
      "batch: 2112, loss: 0.5845978260040283\n",
      "batch: 2113, loss: 0.5843374133110046\n",
      "batch: 2114, loss: 0.5840619206428528\n",
      "batch: 2115, loss: 0.5837723016738892\n",
      "batch: 2116, loss: 0.5834702253341675\n",
      "batch: 2117, loss: 0.583156943321228\n",
      "batch: 2118, loss: 0.5828335285186768\n",
      "batch: 2119, loss: 0.5825012922286987\n",
      "batch: 2120, loss: 0.5821607708930969\n",
      "batch: 2121, loss: 0.5818129777908325\n",
      "batch: 2122, loss: 0.5814589858055115\n",
      "batch: 2123, loss: 0.5810990333557129\n",
      "batch: 2124, loss: 0.5807338953018188\n",
      "batch: 2125, loss: 0.5803641676902771\n",
      "batch: 2126, loss: 0.7605689764022827\n",
      "batch: 2127, loss: 0.821152925491333\n",
      "batch: 2128, loss: 0.8214383125305176\n",
      "batch: 2129, loss: 0.8216283917427063\n",
      "batch: 2130, loss: 0.5792279839515686\n",
      "batch: 2131, loss: 0.5791131258010864\n",
      "batch: 2132, loss: 0.5789687037467957\n",
      "batch: 2133, loss: 0.5787977576255798\n",
      "batch: 2134, loss: 0.57860267162323\n",
      "batch: 2135, loss: 0.5783863067626953\n",
      "batch: 2136, loss: 0.5781508088111877\n",
      "batch: 2137, loss: 0.7006643414497375\n",
      "batch: 2138, loss: 0.8237147331237793\n",
      "batch: 2139, loss: 0.8239034414291382\n",
      "batch: 2140, loss: 0.8240061402320862\n",
      "batch: 2141, loss: 0.8240314722061157\n",
      "batch: 2142, loss: 0.8239871859550476\n",
      "batch: 2143, loss: 0.8238807916641235\n",
      "batch: 2144, loss: 0.8237178325653076\n",
      "batch: 2145, loss: 0.8235047459602356\n",
      "batch: 2146, loss: 0.8232455253601074\n",
      "batch: 2147, loss: 0.8229458928108215\n",
      "batch: 2148, loss: 0.7615922093391418\n",
      "batch: 2149, loss: 0.5788067579269409\n",
      "batch: 2150, loss: 0.6701344847679138\n",
      "batch: 2151, loss: 0.8217958211898804\n",
      "batch: 2152, loss: 0.821530818939209\n",
      "batch: 2153, loss: 0.8212257027626038\n",
      "batch: 2154, loss: 0.8208847045898438\n",
      "batch: 2155, loss: 0.8205115795135498\n",
      "batch: 2156, loss: 0.8201090693473816\n",
      "batch: 2157, loss: 0.8196807503700256\n",
      "batch: 2158, loss: 0.8192290663719177\n",
      "batch: 2159, loss: 0.8187564611434937\n",
      "batch: 2160, loss: 0.8182650208473206\n",
      "batch: 2161, loss: 0.8177567720413208\n",
      "batch: 2162, loss: 0.817233681678772\n",
      "batch: 2163, loss: 0.8166971206665039\n",
      "batch: 2164, loss: 0.8161483407020569\n",
      "batch: 2165, loss: 0.7287713289260864\n",
      "batch: 2166, loss: 0.5844917893409729\n",
      "batch: 2167, loss: 0.5848255753517151\n",
      "batch: 2168, loss: 0.5850844383239746\n",
      "batch: 2169, loss: 0.7282768487930298\n",
      "batch: 2170, loss: 0.8138390779495239\n",
      "batch: 2171, loss: 0.642656147480011\n",
      "batch: 2172, loss: 0.5858712196350098\n",
      "batch: 2173, loss: 0.585993766784668\n",
      "batch: 2174, loss: 0.5860624313354492\n",
      "batch: 2175, loss: 0.5860826373100281\n",
      "batch: 2176, loss: 0.586059033870697\n",
      "batch: 2177, loss: 0.5859960913658142\n",
      "batch: 2178, loss: 0.5858976244926453\n",
      "batch: 2179, loss: 0.5857673287391663\n",
      "batch: 2180, loss: 0.5856084227561951\n",
      "batch: 2181, loss: 0.5854236483573914\n",
      "batch: 2182, loss: 0.5852158069610596\n",
      "batch: 2183, loss: 0.5849871635437012\n",
      "batch: 2184, loss: 0.584739625453949\n",
      "batch: 2185, loss: 0.5844755172729492\n",
      "batch: 2186, loss: 0.757626473903656\n",
      "batch: 2187, loss: 0.8157168626785278\n",
      "batch: 2188, loss: 0.6128355860710144\n",
      "batch: 2189, loss: 0.5836630463600159\n",
      "batch: 2190, loss: 0.5834751129150391\n",
      "batch: 2191, loss: 0.5832644104957581\n",
      "batch: 2192, loss: 0.5830333232879639\n",
      "batch: 2193, loss: 0.582784116268158\n",
      "batch: 2194, loss: 0.5825181603431702\n",
      "batch: 2195, loss: 0.5822377800941467\n",
      "batch: 2196, loss: 0.5819439888000488\n",
      "batch: 2197, loss: 0.7001537680625916\n",
      "batch: 2198, loss: 0.8190111517906189\n",
      "batch: 2199, loss: 0.6704562306404114\n",
      "batch: 2200, loss: 0.5810012221336365\n",
      "batch: 2201, loss: 0.5808003544807434\n",
      "batch: 2202, loss: 0.5805783271789551\n",
      "batch: 2203, loss: 0.5803372859954834\n",
      "batch: 2204, loss: 0.5800792574882507\n",
      "batch: 2205, loss: 0.579805850982666\n",
      "batch: 2206, loss: 0.5795186161994934\n",
      "batch: 2207, loss: 0.5792191624641418\n",
      "batch: 2208, loss: 0.5789085030555725\n",
      "batch: 2209, loss: 0.57858806848526\n",
      "batch: 2210, loss: 0.5782585740089417\n",
      "batch: 2211, loss: 0.5779213309288025\n",
      "batch: 2212, loss: 0.7007089257240295\n",
      "batch: 2213, loss: 0.8242306113243103\n",
      "batch: 2214, loss: 0.8245139718055725\n",
      "batch: 2215, loss: 0.8247020244598389\n",
      "batch: 2216, loss: 0.8248038291931152\n",
      "batch: 2217, loss: 0.8248287439346313\n",
      "batch: 2218, loss: 0.607833206653595\n",
      "batch: 2219, loss: 0.5768424868583679\n",
      "batch: 2220, loss: 0.8248308897018433\n",
      "batch: 2221, loss: 0.7938100099563599\n",
      "batch: 2222, loss: 0.5768775343894958\n",
      "batch: 2223, loss: 0.5768873691558838\n",
      "batch: 2224, loss: 0.5768554210662842\n",
      "batch: 2225, loss: 0.5767857432365417\n",
      "batch: 2226, loss: 0.5766821503639221\n",
      "batch: 2227, loss: 0.5765480995178223\n",
      "batch: 2228, loss: 0.5763866901397705\n",
      "batch: 2229, loss: 0.7320778369903564\n",
      "batch: 2230, loss: 0.8257969617843628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 2231, loss: 0.8259029984474182\n",
      "batch: 2232, loss: 0.8259314298629761\n",
      "batch: 2233, loss: 0.6072171926498413\n",
      "batch: 2234, loss: 0.5759783983230591\n",
      "batch: 2235, loss: 0.8259414434432983\n",
      "batch: 2236, loss: 0.8259211778640747\n",
      "batch: 2237, loss: 0.8258357644081116\n",
      "batch: 2238, loss: 0.7633017301559448\n",
      "batch: 2239, loss: 0.5762625932693481\n",
      "batch: 2240, loss: 0.6697469353675842\n",
      "batch: 2241, loss: 0.8253459930419922\n",
      "batch: 2242, loss: 0.8252061605453491\n",
      "batch: 2243, loss: 0.8250131011009216\n",
      "batch: 2244, loss: 0.6698203682899475\n",
      "batch: 2245, loss: 0.577012300491333\n",
      "batch: 2246, loss: 0.7626003623008728\n",
      "batch: 2247, loss: 0.8242682814598083\n",
      "batch: 2248, loss: 0.6082386374473572\n",
      "batch: 2249, loss: 0.5775261521339417\n",
      "batch: 2250, loss: 0.577593207359314\n",
      "batch: 2251, loss: 0.5776111483573914\n",
      "batch: 2252, loss: 0.5775871872901917\n",
      "batch: 2253, loss: 0.5775246620178223\n",
      "batch: 2254, loss: 0.5774256587028503\n",
      "batch: 2255, loss: 0.5772965550422668\n",
      "batch: 2256, loss: 0.5771396160125732\n",
      "batch: 2257, loss: 0.5769574046134949\n",
      "batch: 2258, loss: 0.5767524838447571\n",
      "batch: 2259, loss: 0.7319381237030029\n",
      "batch: 2260, loss: 0.8254228234291077\n",
      "batch: 2261, loss: 0.6385630965232849\n",
      "batch: 2262, loss: 0.5761072635650635\n",
      "batch: 2263, loss: 0.5759578347206116\n",
      "batch: 2264, loss: 0.5757827758789062\n",
      "batch: 2265, loss: 0.5755841732025146\n",
      "batch: 2266, loss: 0.6067789196968079\n",
      "batch: 2267, loss: 0.8269692659378052\n",
      "batch: 2268, loss: 0.7641208171844482\n",
      "batch: 2269, loss: 0.5748805999755859\n",
      "batch: 2270, loss: 0.5747423768043518\n",
      "batch: 2271, loss: 0.5745782256126404\n",
      "batch: 2272, loss: 0.5743895173072815\n",
      "batch: 2273, loss: 0.5741803646087646\n",
      "batch: 2274, loss: 0.5739493370056152\n",
      "batch: 2275, loss: 0.5737015604972839\n",
      "batch: 2276, loss: 0.5734381079673767\n",
      "batch: 2277, loss: 0.5731603503227234\n",
      "batch: 2278, loss: 0.5728698372840881\n",
      "batch: 2279, loss: 0.5725679397583008\n",
      "batch: 2280, loss: 0.8306894302368164\n",
      "batch: 2281, loss: 0.7986155152320862\n",
      "batch: 2282, loss: 0.5718626379966736\n",
      "batch: 2283, loss: 0.5716739892959595\n",
      "batch: 2284, loss: 0.5714642405509949\n",
      "batch: 2285, loss: 0.5712342262268066\n",
      "batch: 2286, loss: 0.5709872245788574\n",
      "batch: 2287, loss: 0.5707244873046875\n",
      "batch: 2288, loss: 0.5704479813575745\n",
      "batch: 2289, loss: 0.5701586008071899\n",
      "batch: 2290, loss: 0.5698580145835876\n",
      "batch: 2291, loss: 0.7680425047874451\n",
      "batch: 2292, loss: 0.8345338106155396\n",
      "batch: 2293, loss: 0.8347591757774353\n",
      "batch: 2294, loss: 0.8348935842514038\n",
      "batch: 2295, loss: 0.568980872631073\n",
      "batch: 2296, loss: 0.568904459476471\n",
      "batch: 2297, loss: 0.5687956213951111\n",
      "batch: 2298, loss: 0.6353350281715393\n",
      "batch: 2299, loss: 0.8355530500411987\n",
      "batch: 2300, loss: 0.7354470491409302\n",
      "batch: 2301, loss: 0.5683919191360474\n",
      "batch: 2302, loss: 0.5683067440986633\n",
      "batch: 2303, loss: 0.568190336227417\n",
      "batch: 2304, loss: 0.735621988773346\n",
      "batch: 2305, loss: 0.8363152742385864\n",
      "batch: 2306, loss: 0.8363794684410095\n",
      "batch: 2307, loss: 0.8363684415817261\n",
      "batch: 2308, loss: 0.8362898230552673\n",
      "batch: 2309, loss: 0.836150586605072\n",
      "batch: 2310, loss: 0.8359566926956177\n",
      "batch: 2311, loss: 0.8357138633728027\n",
      "batch: 2312, loss: 0.8354267477989197\n",
      "batch: 2313, loss: 0.8351002335548401\n",
      "batch: 2314, loss: 0.83473801612854\n",
      "batch: 2315, loss: 0.8343439102172852\n",
      "batch: 2316, loss: 0.8339210748672485\n",
      "batch: 2317, loss: 0.833469569683075\n",
      "batch: 2318, loss: 0.8330010771751404\n",
      "batch: 2319, loss: 0.8325089812278748\n",
      "batch: 2320, loss: 0.8319981098175049\n",
      "batch: 2321, loss: 0.8314708471298218\n",
      "batch: 2322, loss: 0.8309285640716553\n",
      "batch: 2323, loss: 0.830373227596283\n",
      "batch: 2324, loss: 0.8298059105873108\n",
      "batch: 2325, loss: 0.5733857750892639\n",
      "batch: 2326, loss: 0.6056243181228638\n",
      "batch: 2327, loss: 0.8283766508102417\n",
      "batch: 2328, loss: 0.7645648121833801\n",
      "batch: 2329, loss: 0.5746841430664062\n",
      "batch: 2330, loss: 0.5749286413192749\n",
      "batch: 2331, loss: 0.5751070380210876\n",
      "batch: 2332, loss: 0.5752282738685608\n",
      "batch: 2333, loss: 0.5752965211868286\n",
      "batch: 2334, loss: 0.5753190517425537\n",
      "batch: 2335, loss: 0.575296938419342\n",
      "batch: 2336, loss: 0.7639440298080444\n",
      "batch: 2337, loss: 0.8268780708312988\n",
      "batch: 2338, loss: 0.6066888570785522\n",
      "batch: 2339, loss: 0.5752358436584473\n",
      "batch: 2340, loss: 0.5751927495002747\n",
      "batch: 2341, loss: 0.6065953969955444\n",
      "batch: 2342, loss: 0.8271258473396301\n",
      "batch: 2343, loss: 0.7641259431838989\n",
      "batch: 2344, loss: 0.5749690532684326\n",
      "batch: 2345, loss: 0.574923574924469\n",
      "batch: 2346, loss: 0.5748419761657715\n",
      "batch: 2347, loss: 0.5747277140617371\n",
      "batch: 2348, loss: 0.5745840668678284\n",
      "batch: 2349, loss: 0.5744141936302185\n",
      "batch: 2350, loss: 0.574220597743988\n",
      "batch: 2351, loss: 0.5740057229995728\n",
      "batch: 2352, loss: 0.573771595954895\n",
      "batch: 2353, loss: 0.7971125841140747\n",
      "batch: 2354, loss: 0.8292933702468872\n",
      "batch: 2355, loss: 0.5732213258743286\n",
      "batch: 2356, loss: 0.5730775594711304\n",
      "batch: 2357, loss: 0.5729105472564697\n",
      "batch: 2358, loss: 0.572715699672699\n",
      "batch: 2359, loss: 0.5725002884864807\n",
      "batch: 2360, loss: 0.5722668170928955\n",
      "batch: 2361, loss: 0.5720162987709045\n",
      "batch: 2362, loss: 0.7015472054481506\n",
      "batch: 2363, loss: 0.8316451907157898\n",
      "batch: 2364, loss: 0.8318474888801575\n",
      "batch: 2365, loss: 0.8319603800773621\n",
      "batch: 2366, loss: 0.832001805305481\n",
      "batch: 2367, loss: 0.831967294216156\n",
      "batch: 2368, loss: 0.8318476676940918\n",
      "batch: 2369, loss: 0.8317029476165771\n",
      "batch: 2370, loss: 0.8314814567565918\n",
      "batch: 2371, loss: 0.798812210559845\n",
      "batch: 2372, loss: 0.5720492601394653\n",
      "batch: 2373, loss: 0.5722078680992126\n",
      "batch: 2374, loss: 0.5723061561584473\n",
      "batch: 2375, loss: 0.5723559856414795\n",
      "batch: 2376, loss: 0.5723607540130615\n",
      "batch: 2377, loss: 0.7014577984809875\n",
      "batch: 2378, loss: 0.8306305408477783\n",
      "batch: 2379, loss: 0.8305956125259399\n",
      "batch: 2380, loss: 0.8304759860038757\n",
      "batch: 2381, loss: 0.8303290605545044\n",
      "batch: 2382, loss: 0.8301190137863159\n",
      "batch: 2383, loss: 0.8298595547676086\n",
      "batch: 2384, loss: 0.829567551612854\n",
      "batch: 2385, loss: 0.8292115330696106\n",
      "batch: 2386, loss: 0.8288300037384033\n",
      "batch: 2387, loss: 0.8284505605697632\n",
      "batch: 2388, loss: 0.8280065059661865\n",
      "batch: 2389, loss: 0.8274908065795898\n",
      "batch: 2390, loss: 0.826992928981781\n",
      "batch: 2391, loss: 0.8265650272369385\n",
      "batch: 2392, loss: 0.8259334564208984\n",
      "batch: 2393, loss: 0.8253451585769653\n",
      "batch: 2394, loss: 0.8247833847999573\n",
      "batch: 2395, loss: 0.8241636753082275\n",
      "batch: 2396, loss: 0.823307454586029\n",
      "batch: 2397, loss: 0.8226583003997803\n",
      "batch: 2398, loss: 0.6092461347579956\n",
      "batch: 2399, loss: 0.5794159173965454\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "\n",
    "        data = data.to(device).float()\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        outputs = model(data.permute(0, 2, 1))    \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        print(f'batch: {i}, loss: {loss.item()}')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bbb96f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
